{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "kr_jp = load_from_disk('../data/kr_jp_aihub')\n",
    "kr_cn = load_from_disk('../data/kr_cn_aihub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_jp_train = kr_jp['train'].to_pandas()\n",
    "pd_jp_validation = kr_jp['validation'].to_pandas()\n",
    "pd_cn_train = kr_cn['train'].to_pandas()\n",
    "pd_cn_validation = kr_cn['validation'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_jp_train.insert(0, 'lang', 'jp')\n",
    "pd_cn_train.insert(0, 'lang', 'cn')\n",
    "pd_jp_validation.insert(0, 'lang', 'jp')\n",
    "pd_cn_validation.insert(0, 'lang', 'cn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = pd_jp_train.append(pd_cn_train)\n",
    "pd_validation = pd_jp_validation.append(pd_cn_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train['text'] = pd_train['text'].apply(lambda x: x.replace(' ', ''))\n",
    "pd_validation['text'] = pd_validation['text'].apply(lambda x: x.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = pd_train[pd_train.duplicated(subset=['text'], keep=False)]\n",
    "pd_validation = pd_validation[pd_validation.duplicated(subset=['text'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1675662it [01:40, 16691.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_dict = {}\n",
    "\n",
    "for idx, row in tqdm(pd_train.iterrows()):\n",
    "  if row['text'] not in train_dict.keys():\n",
    "    train_dict[row['text']] = {}\n",
    "  train_dict[row['text']][row['lang']] = row['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26322it [00:01, 16814.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "validation_dict = {}\n",
    "\n",
    "for idx, row in tqdm(pd_validation.iterrows()):\n",
    "  if row['text'] not in validation_dict.keys():\n",
    "    validation_dict[row['text']] = {}\n",
    "  validation_dict[row['text']][row['lang']] = row['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_train = []\n",
    "del_validation = []\n",
    "for key in train_dict.keys():\n",
    "  if len(train_dict[key]) != 2:\n",
    "    del_train.append(key)\n",
    "for key in validation_dict.keys():\n",
    "  if len(validation_dict[key]) != 2:\n",
    "    del_validation.append(key)\n",
    "for key in del_train:\n",
    "  del train_dict[key]\n",
    "for key in del_validation:\n",
    "  del validation_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837828"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {\n",
    "  'id': [],\n",
    "  'title': [],\n",
    "  'text': [],\n",
    "  'target': []\n",
    "}\n",
    "\n",
    "validation = {\n",
    "  'id': [],\n",
    "  'title': [],\n",
    "  'text': [],\n",
    "  'target': []\n",
    "}\n",
    "\n",
    "for idx, key in enumerate(train_dict.keys()):\n",
    "  train['id'].append(str(idx))\n",
    "  train['title'].append('aihub')\n",
    "  train['text'].append(train_dict[key]['jp'])\n",
    "  train['target'].append(train_dict[key]['cn'])\n",
    "\n",
    "for idx, key in enumerate(validation_dict.keys()):\n",
    "  validation['id'].append(str(idx))\n",
    "  validation['title'].append('aihub')\n",
    "  validation['text'].append(validation_dict[key]['jp'])\n",
    "  validation['target'].append(validation_dict[key]['cn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_datasets = Dataset.from_dict(train)\n",
    "validation_datasets = Dataset.from_dict(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_cn_datasets = DatasetDict({\n",
    "  'train': train_datasets,\n",
    "  'validation': validation_datasets\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_cn_datasets.save_to_disk('../data/jp_cn_aihub')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c19bef1cbba8a75632abea5cc212fd678f0f19ee8666d032b381aa1ce8310bb2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
