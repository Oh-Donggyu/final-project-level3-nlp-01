{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset news_commentary (/home/ubuntu/.cache/huggingface/datasets/news_commentary/en-zh/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40418f5bfc5d4e298753890228128b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opus_infopankki (/home/ubuntu/.cache/huggingface/datasets/opus_infopankki/en-zh/1.0.0/430391369129066d03e14183f833034a1245927a2d5846affa6d7005dee1326f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d642d0e0ed4818a590998f0ca20751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wmt20_mlqe_task2 (/home/ubuntu/.cache/huggingface/datasets/wmt20_mlqe_task2/en-zh/1.1.0/2e4959da98bf6a84800ae70a5b7cee59df45b3fc326c9ccd6daa9e4b7811af86)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08723adb3555440f9f2d2881e676071d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wmt20_mlqe_task1 (/home/ubuntu/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/eee738b894fe8fe7e88bb492f4dee78ae811f645562a8afd0d053f50c7ce9f49)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87073b21ec444bf8d1ea3df8653ef4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ted_iwlst2013 (/home/ubuntu/.cache/huggingface/datasets/ted_iwlst2013/en-zh/1.1.0/769086006155211ed7233545de12bce6fe41e1c71f509a3f062e294cb3c00e99)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae26cceca19483b8131a6581d7d113d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: alt/alt-parallel\n",
      "Reusing dataset alt (/home/ubuntu/.cache/huggingface/datasets/alt/alt-parallel/1.0.0/e784a3f2a9f6bdf277940de6cc9d700eab852896cd94aad4233caf26008da9ed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24469a5ce4d4ef3b57f5bd9e7ddf8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "news = load_dataset('news_commentary', 'en-zh')\n",
    "opus = load_dataset('opus_infopankki', 'en-zh')\n",
    "wmt20_2 = load_dataset('wmt20_mlqe_task2', 'en-zh')\n",
    "wmt20_1 = load_dataset('wmt20_mlqe_task1', 'en-zh')\n",
    "ted = load_dataset('ted_iwlst2013', 'en-zh')\n",
    "alt = load_dataset('alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset un_pc (/home/ubuntu/.cache/huggingface/datasets/un_pc/en-zh/1.0.0/1360070a820db42f7427f5a98416dd3a1c956ae069b994bf2ec0b83ae16dcaee)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac48657fcc64ba4a37a6f0a9b34d22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "un_pc = load_dataset('un_pc', 'en-zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 69206\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 29907\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation', 'src_tags', 'mt_tags', 'pe', 'hter', 'alignments'],\n",
      "        num_rows: 7000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation', 'src_tags', 'mt_tags', 'pe', 'hter', 'alignments'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation', 'src_tags', 'mt_tags', 'pe', 'hter', 'alignments'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['segid', 'translation', 'scores', 'mean', 'z_scores', 'z_mean', 'model_score', 'doc_id', 'nmt_output', 'word_probas'],\n",
      "        num_rows: 7000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['segid', 'translation', 'scores', 'mean', 'z_scores', 'z_mean', 'model_score', 'doc_id', 'nmt_output', 'word_probas'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['segid', 'translation', 'scores', 'mean', 'z_scores', 'z_mean', 'model_score', 'doc_id', 'nmt_output', 'word_probas'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 154579\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
      "        num_rows: 18094\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
      "        num_rows: 1004\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
      "        num_rows: 1019\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 17451549\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(news, opus, wmt20_2, wmt20_1, ted, alt, un_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\n",
    "  'id': [],\n",
    "  'title': [],\n",
    "  'text': [],\n",
    "  'target': []\n",
    "}\n",
    "\n",
    "validation_dict = {\n",
    "  'id': [],\n",
    "  'title': [],\n",
    "  'text': [],\n",
    "  'target': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in news['train']:\n",
    "  train_dict['id'].append(str(data['id']))\n",
    "  train_dict['title'].append('news_commentary')\n",
    "  train_dict['text'].append(data['translation']['zh'])\n",
    "  train_dict['target'].append(data['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(opus['train']):\n",
    "  train_dict['id'].append(str(idx))\n",
    "  train_dict['title'].append('opus_infopankki')\n",
    "  train_dict['text'].append(data['translation']['zh'])\n",
    "  train_dict['target'].append(data['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in wmt20_1['train']:\n",
    "  train_dict['id'].append(str(data['segid']))\n",
    "  train_dict['title'].append('wmt20_mlqe_task1')\n",
    "  train_dict['text'].append(data['translation']['zh'])\n",
    "  train_dict['target'].append(data['translation']['en'])\n",
    "for data in wmt20_1['test']:\n",
    "  validation_dict['id'].append(str(data['segid']))\n",
    "  validation_dict['title'].append('wmt20_mlqe_task1')\n",
    "  validation_dict['text'].append(data['translation']['zh'])\n",
    "  validation_dict['target'].append(data['translation']['en'])\n",
    "for data in wmt20_1['validation']:\n",
    "  validation_dict['id'].append(str(data['segid']))\n",
    "  validation_dict['title'].append('wmt20_mlqe_task1')\n",
    "  validation_dict['text'].append(data['translation']['zh'])\n",
    "  validation_dict['target'].append(data['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(wmt20_2['train']):\n",
    "  train_dict['id'].append(str(idx))\n",
    "  train_dict['title'].append('wmt20_mlqe_task2')\n",
    "  train_dict['text'].append(data['translation']['zh'])\n",
    "  train_dict['target'].append(data['translation']['en'])\n",
    "for idx, data in enumerate(wmt20_2['test']):\n",
    "  validation_dict['id'].append(str(idx))\n",
    "  validation_dict['title'].append('wmt20_mlqe_task2')\n",
    "  validation_dict['text'].append(data['translation']['zh'])\n",
    "  validation_dict['target'].append(data['translation']['en'])\n",
    "for idx, data in enumerate(wmt20_2['validation']):\n",
    "  validation_dict['id'].append(str(idx))\n",
    "  validation_dict['title'].append('wmt20_mlqe_task2')\n",
    "  validation_dict['text'].append(data['translation']['zh'])\n",
    "  validation_dict['target'].append(data['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in ted['train']:\n",
    "  train_dict['id'].append(str(data['id']))\n",
    "  train_dict['title'].append('ted_iwlst2013')\n",
    "  train_dict['text'].append(data['translation']['zh'])\n",
    "  train_dict['target'].append(data['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in alt['train']:\n",
    "  train_dict['id'].append(str(data['SNT.URLID.SNTID']))\n",
    "  train_dict['title'].append('alt')\n",
    "  train_dict['text'].append(data['translation']['zh'])\n",
    "  train_dict['target'].append(data['translation']['en'])\n",
    "for data in alt['test']:\n",
    "  validation_dict['id'].append(str(data['SNT.URLID.SNTID']))\n",
    "  validation_dict['title'].append('alt')\n",
    "  validation_dict['text'].append(data['translation']['zh'])\n",
    "  validation_dict['target'].append(data['translation']['en'])\n",
    "for data in alt['validation']:\n",
    "  validation_dict['id'].append(str(data['SNT.URLID.SNTID']))\n",
    "  validation_dict['title'].append('alt')\n",
    "  validation_dict['text'].append(data['translation']['zh'])\n",
    "  validation_dict['target'].append(data['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17451549it [18:19, 15874.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for idx, data in tqdm(enumerate(un_pc['train'])):\n",
    "  train_dict['id'].append(str(idx))\n",
    "  train_dict['title'].append('un_pc')\n",
    "  train_dict['text'].append(data['translation']['zh'])\n",
    "  train_dict['target'].append(data['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "validation_dataset = Dataset.from_dict(validation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_equal(example):\n",
    "  if example['text'] == example['target']:\n",
    "    return False\n",
    "  return True\n",
    "\n",
    "new_train_dataset = train_dataset.filter(del_equal, num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_train_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "data_dict = {\n",
    "  'train': new_train_dataset,\n",
    "  'validation': validation_dataset\n",
    "}\n",
    "\n",
    "zh_en = DatasetDict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60366c0b40694f5b832963723a783f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split validation to the Hub.\n",
      "The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25bd83ab6f5489b9a6cbffb09476e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zh_en.push_to_hub('deCyma/zh_en', private=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c19bef1cbba8a75632abea5cc212fd678f0f19ee8666d032b381aa1ce8310bb2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
