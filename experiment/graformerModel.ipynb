{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lightweight/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at THUMT/mGPT were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoModel, AutoTokenizer, MT5Tokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "ENCODER_MODEL = \"bert-base-multilingual-cased\"\n",
    "DECODER_MODEL = \"THUMT/mGPT\"\n",
    "\n",
    "mBERT = AutoModel.from_pretrained(ENCODER_MODEL)\n",
    "mBERT_tokenizer = AutoTokenizer.from_pretrained(ENCODER_MODEL)\n",
    "\n",
    "# gpt_config = AutoConfig.from_pretrained(DECODER_MODEL) , config=gpt_config\n",
    "mGPT = AutoModel.from_pretrained(DECODER_MODEL)\n",
    "mGPT_tokenizer = MT5Tokenizer.from_pretrained(DECODER_MODEL)\n",
    "mGPT.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mGPT memory usage: 2263.02 MB\n",
      "mBERT memory usage: 711.42 MB\n"
     ]
    }
   ],
   "source": [
    "mem_params = sum([param.nelement()*param.element_size() for param in mGPT.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in mGPT.buffers()])\n",
    "mem = mem_params + mem_bufs\n",
    "print(f'mGPT memory usage: {mem/1e6:.2f} MB')\n",
    "\n",
    "mem_params = sum([param.nelement()*param.element_size() for param in mBERT.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in mBERT.buffers()])\n",
    "mem = mem_params + mem_bufs\n",
    "print(f'mBERT memory usage: {mem/1e6:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mGPT Tokenizer info\n",
      "vocab size: 250100\n",
      "special tokens: ['</s>', '<unk>', '<pad>']\n",
      "['▁이', '순', '신', '은', '조선', '중', '기의', '무', '신', '이다']\n",
      "['▁아', '버', '지가', '방', '에', '들어', '가', '신', '다'] \n",
      "\n",
      "mBERT Tokenizer info\n",
      "vocab size: 119547\n",
      "special tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "['이', '##순', '##신', '##은', '##조', '##선', '##중', '##기의', '##무', '##신', '##이다']\n",
      "['아버지', '##가', '##방', '##에', '##들어', '##가', '##신', '##다']\n"
     ]
    }
   ],
   "source": [
    "print(\"mGPT Tokenizer info\")\n",
    "print(f\"vocab size: {len(mGPT_tokenizer)}\")\n",
    "print(f\"special tokens: {mGPT_tokenizer.all_special_tokens}\")  # 이 친구 bos 토큰이 없음.\n",
    "print(mGPT_tokenizer.tokenize(\"이순신은조선중기의무신이다\"))\n",
    "print(mGPT_tokenizer.tokenize(\"아버지가방에들어가신다\"), '\\n')\n",
    "\n",
    "print(\"mBERT Tokenizer info\")\n",
    "print(f\"vocab size: {len(mBERT_tokenizer)}\")\n",
    "print(f\"special tokens: {mBERT_tokenizer.all_special_tokens}\")\n",
    "print(mBERT_tokenizer.tokenize(\"이순신은조선중기의무신이다\"))\n",
    "print(mBERT_tokenizer.tokenize(\"아버지가방에들어가신다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\"additional_special_tokens\": [\"<s>\"]}\n",
    "mGPT_tokenizer.add_special_tokens(special_tokens_dict=special_tokens_dict)\n",
    "mGPT.resize_token_embeddings(len(mGPT_tokenizer))\n",
    "mGPT_tokenizer.bos_token = \"<s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d15fbe40f58f4a20\n",
      "Reusing dataset csv (/opt/ml/.cache/huggingface/datasets/csv/default-d15fbe40f58f4a20/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f78ec306b34e92a8f54420b407150f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-67c6f6653f5a37c1\n",
      "Reusing dataset csv (/opt/ml/.cache/huggingface/datasets/csv/default-67c6f6653f5a37c1/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989d95ff0a294d6bbe65bfdadef225f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "data_dir = Path(\"/opt/ml/final-project-level3-nlp-01/data/ko-ja\")\n",
    "folder_list = os.listdir(data_dir)\n",
    "\n",
    "dataset_dict = dict()\n",
    "\n",
    "for folder in folder_list:\n",
    "    data = load_dataset(\"csv\", data_files=[str(p) for p in data_dir.joinpath(folder).glob(\"*.csv\")])\n",
    "    dataset_dict[folder] = data['train']\n",
    "\n",
    "raw_dataset = DatasetDict(dataset_dict)\n",
    "train_set = raw_dataset[\"Training\"]\n",
    "valid_set = raw_dataset[\"Validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/csv/default-d15fbe40f58f4a20/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-21ef7ec33d993a64.arrow\n",
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/csv/default-d15fbe40f58f4a20/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-913a4dd487b68236.arrow\n",
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/csv/default-d15fbe40f58f4a20/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-abc9db3d2c1941a2.arrow\n",
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/csv/default-d15fbe40f58f4a20/0.0.0/bf68