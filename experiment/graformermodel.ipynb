{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoModel, AutoTokenizer, MT5Tokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "ENCODER_MODEL = \"bert-base-multilingual-cased\"\n",
    "DECODER_MODEL = \"THUMT/mGPT\"\n",
    "\n",
    "mBERT = AutoModel.from_pretrained(ENCODER_MODEL)\n",
    "mBERT_tokenizer = AutoTokenizer.from_pretrained(ENCODER_MODEL)\n",
    "\n",
    "# gpt_config = AutoConfig.from_pretrained(DECODER_MODEL) , config=gpt_config\n",
    "mGPT = AutoModel.from_pretrained(DECODER_MODEL)\n",
    "mGPT_tokenizer = MT5Tokenizer.from_pretrained(DECODER_MODEL)\n",
    "mGPT.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_params = sum([param.nelement()*param.element_size() for param in mGPT.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in mGPT.buffers()])\n",
    "mem = mem_params + mem_bufs\n",
    "print(f'mGPT memory usage: {mem/1e6:.2f} MB')\n",
    "\n",
    "mem_params = sum([param.nelement()*param.element_size() for param in mBERT.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in mBERT.buffers()])\n",
    "mem = mem_params + mem_bufs\n",
    "print(f'mBERT memory usage: {mem/1e6:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mGPT Tokenizer info\")\n",
    "print(f\"vocab size: {len(mGPT_tokenizer)}\")\n",
    "print(f\"special tokens: {mGPT_tokenizer.all_special_tokens}\")  # 이 친구 bos 토큰이 없음.\n",
    "print(mGPT_tokenizer.tokenize(\"이순신은조선중기의무신이다\"))\n",
    "print(mGPT_tokenizer.tokenize(\"아버지가방에들어가신다\"), '\\n')\n",
    "\n",
    "print(\"mBERT Tokenizer info\")\n",
    "print(f\"vocab size: {len(mBERT_tokenizer)}\")\n",
    "print(f\"special tokens: {mBERT_tokenizer.all_special_tokens}\")\n",
    "print(mBERT_tokenizer.tokenize(\"이순신은조선중기의무신이다\"))\n",
    "print(mBERT_tokenizer.tokenize(\"아버지가방에들어가신다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\"additional_special_tokens\": [\"<s>\"]}\n",
    "mGPT_tokenizer.add_special_tokens(special_tokens_dict=special_tokens_dict)\n",
    "mGPT.resize_token_embeddings(len(mGPT_tokenizer))\n",
    "mGPT_tokenizer.bos_token = \"<s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "data_dir = Path(\"/opt/ml/final-project-level3-nlp-01/data/ko-ja\")\n",
    "folder_list = os.listdir(data_dir)\n",
    "\n",
    "dataset_dict = dict()\n",
    "\n",
    "for folder in folder_list:\n",
    "    data = load_dataset(\"csv\", data_files=[str(p) for p in data_dir.joinpath(folder).glob(\"*.csv\")])\n",
    "    dataset_dict[folder] = data['train']\n",
    "\n",
    "raw_dataset = DatasetDict(dataset_dict)\n",
    "train_set = raw_dataset[\"Training\"]\n",
    "valid_set = raw_dataset[\"Validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(examples, max_length, max_target_length):\n",
    "\n",
    "    model_inputs = mBERT_tokenizer(examples['한국어'], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    target_sentences = [mGPT_tokenizer.bos_token + ex for ex in examples['일본어']]\n",
    "    decoder_inputs = mGPT_tokenizer(target_sentences, max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs['decoder_input_ids'] = decoder_inputs['input_ids']\n",
    "    model_inputs['decoder_attention_mask'] = decoder_inputs['attention_mask']\n",
    "    model_inputs['labels'] = [ex[1:] for ex in decoder_inputs['input_ids']]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "fn_kwargs = {\"max_length\": 512, \"max_target_length\": 1024}\n",
    "tokenized_train = train_set.map(func, num_proc=5, batched=True, remove_columns=train_set.column_names, fn_kwargs=fn_kwargs)\n",
    "tokenized_valid = valid_set.map(func, num_proc=5, batched=True, remove_columns=valid_set.column_names, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sample_indices = random.choices(range(len(tokenized_valid)), k=1)\n",
    "\n",
    "sample = tokenized_train.select(sample_indices)\n",
    "for s in sample:\n",
    "    print(f\"inputs: {mBERT_tokenizer.decode(s['input_ids'])}\")\n",
    "    print(f\"decoder inputs: {mGPT_tokenizer.decode(s['decoder_input_ids'])}\")\n",
    "    print(f\"labels: {mGPT_tokenizer.decode(s['labels'])}\", \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mBERT.to(device)\n",
    "mGPT.to(device)\n",
    "\n",
    "mBERT.eval()\n",
    "mGPT.eval()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train, batch_size=4, pin_memory=True, shuffle=True, \n",
    "    drop_last=True, num_workers=5, collate_fn=default_data_collator,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    tokenized_valid, batch_size=4, pin_memory=True, shuffle=False, \n",
    "    drop_last=False, num_workers=5, collate_fn=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "bart_enc_config = AutoConfig.from_pretrained(\"facebook/bart-base\")\n",
    "bart_dec_config = deepcopy(bart_enc_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.models.bart.modeling_bart import BartEncoderLayer, BartDecoderLayer\n",
    "\n",
    "class graft_module(nn.Module):\n",
    "    def __init__(self, num_enc_layers=1, num_dec_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.graft_enc_layers = nn.ModuleList([BartEncoderLayer() for _ in range(num_enc_layers)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import models\n",
    "from transformers.models.bart.modeling_bart import _expand_mask\n",
    "train_batch = next(iter(train_dataloader))\n",
    "\n",
    "decoder_pooler = nn.Linear(1024, 768).to(device)\n",
    "graft_enc_layer = BartEncoderLayer(bart_enc_config).to(device)\n",
    "graft_dec_layer = BartDecoderLayer(bart_dec_config).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_batch = {k: t.to(device) for k, t in train_batch.items()}\n",
    "    bert_output = mBERT(train_batch[\"input_ids\"], attention_mask=train_batch[\"attention_mask\"])\n",
    "    gpt_output = mGPT(train_batch[\"decoder_input_ids\"], attention_mask=train_batch[\"decoder_attention_mask\"])\n",
    "\n",
    "    mask = _expand_mask(train_batch['attention_mask'], bert_output.last_hidden_state.dtype)\n",
    "    graft_dec_input = decoder_pooler(gpt_output.last_hidden_state)\n",
    "    dec_mask = _expand_mask(train_batch[\"decoder_attention_mask\"], gpt_output.last_hidden_state.dtype)\n",
    "    print(dec_mask.shape)\n",
    "    \n",
    "    graft_enc_output = graft_enc_layer(bert_output.last_hidden_state, attention_mask=mask, layer_head_mask=None)[0]\n",
    "    graft_dec_output = graft_dec_layer(hidden_states=graft_dec_input, attention_mask=dec_mask, \n",
    "                                       encoder_hidden_states=bert_output.last_hidden_state, encoder_attention_mask=mask, use_cache=False)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output.last_hidden_state.shape, gpt_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
