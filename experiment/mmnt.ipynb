{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4568c14414cede3a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /opt/ml/.cache/huggingface/datasets/csv/default-4568c14414cede3a/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2136509811d34d41884d931258517670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a27617ee11408ca9539240dbb5b75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /opt/ml/.cache/huggingface/datasets/csv/default-4568c14414cede3a/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fc33fe75b44fa8ae27a027304c3636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-13d6237d81cc2388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /opt/ml/.cache/huggingface/datasets/csv/default-13d6237d81cc2388/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8f7b4437b54bdba87b96ff67f81e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543c905e8610499e88be419d62cf01d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /opt/ml/.cache/huggingface/datasets/csv/default-13d6237d81cc2388/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef50f9effee44ed8145d058d9599c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "data_dir = Path(\"/opt/ml/final/data/ko-ja\")\n",
    "folder_list = os.listdir(data_dir)\n",
    "\n",
    "dataset_dict = dict()\n",
    "\n",
    "for folder in folder_list:\n",
    "    data = load_dataset(\"csv\", data_files=[str(p) for p in data_dir.joinpath(folder).glob(\"*.csv\")])\n",
    "    dataset_dict[folder] = data['train']\n",
    "\n",
    "raw_dataset = DatasetDict(dataset_dict)\n",
    "train_set = raw_dataset[\"Training\"]\n",
    "valid_set = raw_dataset[\"Validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'관리번호': 'KO-JA-2020-FOLK-022556', '분야': '문화재/향토/K-FOOD', '한국어': '천생산의 통신바위로 가는 길목 골짜기에 약수터가 자리하고 있으므로 약샘골이라 한다.', '일본어': '天生山の通信岩路に向かう街角の谷間に、薬水場が位置しているため、薬泉谷という。', '한국어_어절수': 10, '일본어_글자수': 36, '길이_분류': 2, '출처': 'http://gumi.grandculture.net/gumi/toc/GC01200066', '수행기관': '플리토'} \n",
      "\n",
      "\n",
      "{'관리번호': 'KO-JA-2020-SOCI-089000', '분야': '사회/노동/복지', '한국어': '하지만, 김 실장은 \"이번 최저임금위원회의 결정이 소득주도성장의 폐기 또는 포기를 의미하는 것으로 오해되지 않았으면 한다\"고 강조했다.', '일본어': 'だが、キム室長は「今回の最低賃金委員会の決定が所得主導成長の廃棄または放棄を意味すると誤解されないように望む」と強調した。', '한국어_어절수': 16, '일본어_글자수': 57, '길이_분류': 3, '출처': None, '수행기관': '플리토'} \n",
      "\n",
      "\n",
      "{'관리번호': 'KO-JA-2020-MEDI-045556', '분야': '의료/보건', '한국어': \"확진자들이 대거 발생할 당시 '자쿠와' 술집 인근에 있었다고 밝힌 인원만 지난 21일 현재 1,000여 명에 달해 지역감염 확산 및 '제2의 이태원 클럽'이 되는 것 우려의 목소리가 커지고 있다.\", '일본어': '多くの感染者が発生した当時、「ザクワ」居酒屋の付近にいたと明らかにした人数だけで21日現在約1,000人に達し、地域感染の拡散及び「第2の梨泰院クラブ」になるのではないかと懸念する声が高まっている。', '한국어_어절수': 28, '일본어_글자수': 84, '길이_분류': 5, '출처': 'http://uci.or.kr/G703:RA101-01101101.20200523050227002:1', '수행기관': '솔트룩스파트너스'} \n",
      "\n",
      "\n",
      "{'관리번호': 'KO-JA-2020-FOLK-030132', '분야': '문화재/향토/K-FOOD', '한국어': '도리사를 품에 안은 태조산 뒤편에 뻗어 내린 산줄기가 낙동강에 닿을 즈음에 자잘하게 널려 퍼진 구릉 산지였다.', '일본어': '桃李寺を懐に抱いた太祖山の裏側に伸びている山茎が洛東江に当たる頃によく散らばった丘陵山地であった。', '한국어_어절수': 16, '일본어_글자수': 48, '길이_분류': 3, '출처': 'http://pocheon.grandculture.net/pocheon/toc/GC05000123', '수행기관': '플리토'} \n",
      "\n",
      "\n",
      "{'관리번호': 'KO-JA-2020-SOCI-057768', '분야': '사회/노동/복지', '한국어': '원심은 원고 일부승소로 판결했지만 총근로시간 산정은 기존 대법원 판례를 따랐다.', '일본어': '原審は、原告の一部勝訴の判決を下したが、総勤労時間の算定は既存の最高裁判所の判例に従った。', '한국어_어절수': 10, '일본어_글자수': 42, '길이_분류': 2, '출처': 'http://www.sedaily.com/NewsView/1YXQW639NM', '수행기관': '플리토'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample_indices = random.choices(range(len(train_set)), k=5)\n",
    "\n",
    "for sample in train_set.select(sample_indices):\n",
    "    print(sample, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at THUMT/mGPT and are newly initialized: ['transformer.h.3.ln_cross_attn.weight', 'transformer.h.20.crossattention.c_proj.bias', 'transformer.h.23.crossattention.c_proj.bias', 'transformer.h.9.crossattention.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.20.crossattention.q_attn.weight', 'transformer.h.11.crossattention.bias', 'transformer.h.15.crossattention.q_attn.weight', 'transformer.h.18.crossattention.masked_bias', 'transformer.h.14.crossattention.q_attn.weight', 'transformer.h.16.crossattention.bias', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.18.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.2.crossattention.bias', 'transformer.h.14.crossattention.c_proj.weight', 'transformer.h.14.crossattention.masked_bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.22.ln_cross_attn.weight', 'transformer.h.15.crossattention.bias', 'transformer.h.16.crossattention.c_attn.weight', 'transformer.h.16.ln_cross_attn.weight', 'transformer.h.22.crossattention.c_proj.weight', 'transformer.h.20.crossattention.c_attn.weight', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.17.crossattention.c_proj.bias', 'transformer.h.18.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.15.crossattention.c_attn.weight', 'transformer.h.12.crossattention.c_proj.weight', 'transformer.h.17.crossattention.masked_bias', 'transformer.h.11.crossattention.masked_bias', 'transformer.h.17.crossattention.c_proj.weight', 'transformer.h.12.crossattention.masked_bias', 'transformer.h.9.ln_cross_attn.weight', 'transformer.h.6.crossattention.masked_bias', 'transformer.h.9.crossattention.masked_bias', 'transformer.h.7.crossattention.masked_bias', 'transformer.h.13.crossattention.c_proj.weight', 'transformer.h.23.crossattention.masked_bias', 'transformer.h.18.crossattention.c_attn.weight', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.22.crossattention.c_proj.bias', 'transformer.h.23.crossattention.c_proj.weight', 'transformer.h.8.crossattention.masked_bias', 'transformer.h.21.ln_cross_attn.weight', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.23.ln_cross_attn.weight', 'transformer.h.18.crossattention.q_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.20.crossattention.masked_bias', 'transformer.h.19.ln_cross_attn.weight', 'transformer.h.12.crossattention.bias', 'transformer.h.19.crossattention.c_proj.weight', 'transformer.h.13.crossattention.bias', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.16.crossattention.masked_bias', 'transformer.h.10.crossattention.masked_bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.12.ln_cross_attn.weight', 'transformer.h.12.crossattention.q_attn.weight', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.17.crossattention.q_attn.weight', 'transformer.h.4.crossattention.bias', 'transformer.h.13.crossattention.masked_bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.21.crossattention.c_proj.bias', 'transformer.h.21.crossattention.masked_bias', 'transformer.h.22.crossattention.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.15.crossattention.masked_bias', 'transformer.h.5.crossattention.bias', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.19.crossattention.masked_bias', 'transformer.h.22.crossattention.q_attn.weight', 'transformer.h.19.crossattention.c_attn.weight', 'transformer.h.23.crossattention.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.21.crossattention.q_attn.weight', 'transformer.h.14.crossattention.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.20.ln_cross_attn.weight', 'transformer.h.13.crossattention.c_proj.bias', 'transformer.h.12.crossattention.c_proj.bias', 'transformer.h.18.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.8.crossattention.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.3.crossattention.masked_bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.0.crossattention.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.15.crossattention.c_proj.weight', 'transformer.h.22.crossattention.c_attn.weight', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.23.crossattention.c_attn.weight', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.14.crossattention.c_attn.weight', 'transformer.h.17.crossattention.c_attn.weight', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.16.crossattention.c_proj.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.17.ln_cross_attn.weight', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.19.crossattention.q_attn.weight', 'transformer.h.21.crossattention.c_proj.weight', 'transformer.h.19.crossattention.c_proj.bias', 'transformer.h.1.crossattention.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.21.crossattention.c_attn.weight', 'transformer.h.20.crossattention.c_proj.weight', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.19.crossattention.bias', 'transformer.h.3.crossattention.bias', 'transformer.h.13.crossattention.q_attn.weight', 'transformer.h.15.ln_cross_attn.weight', 'transformer.h.14.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.6.crossattention.bias', 'transformer.h.13.crossattention.c_attn.weight', 'transformer.h.22.crossattention.masked_bias', 'transformer.h.18.crossattention.bias', 'transformer.h.16.crossattention.c_proj.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.16.crossattention.q_attn.weight', 'transformer.h.14.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.13.ln_cross_attn.weight', 'transformer.h.21.crossattention.bias', 'transformer.h.20.crossattention.bias', 'transformer.h.15.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.10.crossattention.bias', 'transformer.h.12.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.7.crossattention.bias', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.17.crossattention.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.23.crossattention.q_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, MT5Tokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "ENCODER_MODEL = \"bert-base-multilingual-cased\"\n",
    "DECODER_MODEL = \"THUMT/mGPT\"\n",
    "\n",
    "mBERT = AutoModel.from_pretrained(ENCODER_MODEL)\n",
    "mBERT_tokenizer = AutoTokenizer.from_pretrained(ENCODER_MODEL)\n",
    "\n",
    "decoder_config = AutoConfig.from_pretrained(DECODER_MODEL)\n",
    "decoder_config.add_cross_attention=True\n",
    "mGPT = GPT2LMHeadModel.from_pretrained(DECODER_MODEL, config=decoder_config)\n",
    "mGPT_tokenizer = MT5Tokenizer.from_pretrained(DECODER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(examples, max_length):\n",
    "    model_inputs = mBERT_tokenizer(examples['한국어'], max_length, padding=\"max_length\", truncation=True)\n",
    "    labels = mGPT_tokenizer(examples['일본어'], max_length=max_target_length, padding=\"max_target_length\", truncation=True)\n",
    "    pass\n",
    "\n",
    "fn_kwargs = {\"max_length\": 512}\n",
    "tokenized_train = train_set.map(func, num_proc=5, remove_columns=train_set.column_names, fn_kwargs=kn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sample_indices = random.choices(range(len(train_set)), k=5)\n",
    "\n",
    "for sample in train_set.select(sample_indices):\n",
    "    print(sample, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(250100, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mBERT_tokenizer.tokenize(\"이순신은조선중기의무신이다.\"))\n",
    "print(mBERT_tokenizer.tokenize(\"아버지가방에들어가신다.\"))\n",
    "print(mBERT_tokenizer.tokenize(\"被酷禍物を加熱したあと、水蒸気を噴射して酷禍させる配置方式の酷禍炉において、水蒸気に含まれた凝縮水などの不純物が、酷禍不良のみならず、脆弱な被酷禍物を損傷させることになる。\"))\n",
    "print()\n",
    "print(mGPT_tokenizer.tokenize(\"이순신은조선중기의무신이다.\"))\n",
    "print(mGPT_tokenizer.tokenize(\"아버지가방에들어가신다.\"))\n",
    "print(mGPT_tokenizer.tokenize(\"被酷禍物を加熱したあと、水蒸気を噴射して酷禍させる配置方式の酷禍炉において、水蒸気に含まれた凝縮水などの不純物が、酷禍不良のみならず、脆弱な被酷禍物を損傷させることになる。\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8fbe48cf82b777f7f581a3f147c198bc6fbe633068f4ff656ee223b71ce21b8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
