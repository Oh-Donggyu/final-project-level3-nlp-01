{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from packaging import version\n",
    "from torch.nn import init\n",
    "from tensorly.tenalg import multi_mode_dot\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.file_utils import PushToHubMixin\n",
    "from transformers.modeling_utils import (ModuleUtilsMixin,\n",
    "                                         apply_chunking_to_forward, \n",
    "                                         find_pruneable_heads_and_indices,\n",
    "                                         prune_linear_layer)\n",
    "from transformers.models.bart.modeling_bart import BartEncoderLayer, BartDecoderLayer\n",
    "from transformers.activations import gelu_new\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions, \n",
    "    CausalLMOutputWithCrossAttentions\n",
    ")\n",
    "\n",
    "\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric, load_from_disk\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import set_seed, get_cosine_schedule_with_warmup, AdamW\n",
    "\n",
    "from model import GrafomerModel\n",
    "from utils import preprocess_function_with_setting, load_data, postprocess_text, CustomDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TeacherWeightGroup:\n",
    "    teacher_model: nn.Module = None\n",
    "\n",
    "    @classmethod\n",
    "    def set_network(cls, teacher_model: nn.Module):\n",
    "        TeacherWeightGroup.teacher_model = teacher_model\n",
    "\n",
    "    @classmethod\n",
    "    def generate_weight_group(\n",
    "        cls, \n",
    "        weight_class_name: str, \n",
    "        current_layer_index: int, \n",
    "        num_student_layers: int\n",
    "    ):\n",
    "        part, weight_class_name = weight_class_name.split(\".\", 1)\n",
    "        weight_class_name += \".weight\"\n",
    "        weight_instances = list()\n",
    "\n",
    "        if part == \"encoder\":\n",
    "            for instance_name, instance in TeacherWeightGroup.teacher_model.encoder.named_parameters():\n",
    "                if \"attention\" in weight_class_name:\n",
    "                    if weight_class_name in instance_name and 'attention' in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "                else:\n",
    "                    if weight_class_name in instance_name and 'attention' not in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "\n",
    "        elif part == \"decoder\":\n",
    "            for instance_name, instance in TeacherWeightGroup.teacher_model.decoder.named_parameters():\n",
    "                if \"attention\" in weight_class_name:\n",
    "                    if weight_class_name in instance_name and 'attention' in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "                else:\n",
    "                    if weight_class_name in instance_name and 'attention' not in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "        \n",
    "        weight_instances = torch.stack(weight_instances, dim=-1)\n",
    "        teacher_network_layers = weight_instances.size()[-1]\n",
    "        \n",
    "        start = (current_layer_index - 1) * int(teacher_network_layers / num_student_layers)\n",
    "        end = current_layer_index * int(teacher_network_layers / num_student_layers)\n",
    "        return weight_instances[:, :, start:end]\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_bias_group(\n",
    "        cls, \n",
    "        weight_class_name: str, \n",
    "        current_layer_index: int, \n",
    "        num_student_layers: int\n",
    "    ):\n",
    "        part, weight_class_name = weight_class_name.split(\".\", 1)\n",
    "        weight_class_name += \".bias\"\n",
    "        weight_instances = list()\n",
    "\n",
    "        if part == \"encoder\":\n",
    "            for instance_name, instance in TeacherWeightGroup.teacher_model.encoder.named_parameters():\n",
    "                if \"attention\" in weight_class_name:\n",
    "                    if weight_class_name in instance_name and 'attention' in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "                else:\n",
    "                    if weight_class_name in instance_name and 'attention' not in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "\n",
    "        elif part == \"decoder\":\n",
    "            for instance_name, instance in TeacherWeightGroup.teacher_model.decoder.named_parameters():\n",
    "                if \"attention\" in weight_class_name:\n",
    "                    if weight_class_name in instance_name and 'attention' in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "                else:\n",
    "                    if weight_class_name in instance_name and 'attention' not in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "        \n",
    "        weight_instances = torch.stack(weight_instances, dim=-1)\n",
    "        teacher_network_layers = weight_instances.size()[-1]\n",
    "        \n",
    "        start = (current_layer_index - 1) * int(teacher_network_layers / num_student_layers)\n",
    "        end = current_layer_index * int(teacher_network_layers / num_student_layers)\n",
    "        return weight_instances[:, start:end]\n",
    "\n",
    "\n",
    "# new\n",
    "class WeightGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_class_name: str,\n",
    "        current_layer_index: int,\n",
    "        num_student_layers: int,\n",
    "        student_weight_in: int,\n",
    "        student_weight_out: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.subset = TeacherWeightGroup.generate_weight_group(\n",
    "            weight_class_name, current_layer_index, num_student_layers\n",
    "        )\n",
    "        teacher_weight_out, teacher_weight_in, num_adjacent_layers = self.subset.size()\n",
    "\n",
    "        self.W_l = nn.Parameter(torch.empty(num_adjacent_layers, 1))\n",
    "        self.W = nn.Parameter(torch.ones(student_weight_out, student_weight_in))\n",
    "        self.B = nn.Parameter(torch.zeros(student_weight_out, student_weight_in))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights_()\n",
    "    \n",
    "    def init_weights_(self):\n",
    "        init.xavier_uniform_(self.W_l)\n",
    "\n",
    "    def forward(self) -> nn.Parameter :\n",
    "        student_param = self.subset.matmul(self.W_l)\n",
    "        return self.tanh(student_param.squeeze(-1)) * self.W + self.B\n",
    "\n",
    "\n",
    "# New\n",
    "class BiasGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_class_name: str,\n",
    "        current_layer_index: int,\n",
    "        num_student_layers: int,\n",
    "        student_out_features: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.subset = TeacherWeightGroup.generate_bias_group(\n",
    "            weight_class_name, current_layer_index, num_student_layers\n",
    "        )\n",
    "        teacher_out_features, num_adjacent_layers = self.subset.shape\n",
    "\n",
    "        self.W_l = nn.Parameter(torch.empty(num_adjacent_layers, 1))\n",
    "        self.W = nn.Parameter(torch.ones(student_out_features))\n",
    "        self.B = nn.Parameter(torch.zeros(student_out_features))\n",
    "        \n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights_()\n",
    "    \n",
    "    def init_weights_(self):\n",
    "        init.xavier_uniform_(self.W_l)\n",
    "    \n",
    "    def forward(self) -> nn.Parameter :\n",
    "        student_param = self.subset.matmul(self.W_l)\n",
    "        return self.tanh(student_param.squeeze(-1)) * self.W + self.B\n",
    "\n",
    "\n",
    "class StudentLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        weight_class_name: str, \n",
    "        current_layer_index: int, \n",
    "        num_student_layers: int,\n",
    "        in_features: int, \n",
    "        out_features: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight_generator = WeightGenerator(\n",
    "            weight_class_name = weight_class_name, \n",
    "            current_layer_index = current_layer_index, \n",
    "            num_student_layers = num_student_layers, \n",
    "            student_weight_in = in_features, \n",
    "            student_weight_out = out_features,\n",
    "        )\n",
    "        self.bias_generator = BiasGenerator(\n",
    "            weight_class_name = weight_class_name, \n",
    "            current_layer_index = current_layer_index, \n",
    "            num_student_layers = num_student_layers, \n",
    "            student_out_features = out_features,\n",
    "        )\n",
    "            \n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor :\n",
    "\n",
    "        student_weight = self.weight_generator()\n",
    "        student_bias = self.bias_generator()\n",
    "        \n",
    "        return F.linear(inputs, student_weight, student_bias)\n",
    "\n",
    "\n",
    "class StudentMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        current_layer_index,\n",
    "        num_student_layers,\n",
    "        config\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        intermediate_size = config[\"intermediate_size\"]\n",
    "        \n",
    "        self.c_fc = StudentLinear(\n",
    "            \"decoder.mlp.c_fc\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            hidden_size, \n",
    "            intermediate_size\n",
    "        )\n",
    "        self.c_proj = StudentLinear(\n",
    "            \"decoder.mlp.c_proj\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            intermediate_size, \n",
    "            hidden_size\n",
    "        )\n",
    "        self.act = gelu_new\n",
    "        self.dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.c_fc(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.c_proj(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda:0')\n",
    "TeacherWeightGroup.set_network(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate = StudentLinear('encoder.intermediate.dense', 2, 2, 768, 3072)\n",
    "output = StudentLinear('encoder.output.dense', 2, 2, 3072, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentLinear(\n",
       "  (weight_generator): WeightGenerator(\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (bias_generator): BiasGenerator(\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 768),\n",
    "            nn.ReLU(),\n",
    "            # StudentLinear('encoder.intermediate.dense', 2, 2, 768, 3072).to(device),\n",
    "            # StudentLinear('encoder.output.dense', 2, 2, 3072, 768).to(device),\n",
    "            StudentLinear('encoder.intermediate.dense', 2, 2, 768, 3072),\n",
    "            StudentLinear('encoder.output.dense', 2, 2, 3072, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, epochs):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    # progress_bar = tqdm(range(epochs), ncols=100)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 예측(prediction)과 손실(loss) 계산\n",
    "        # X.to(device)\n",
    "        # y.to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        # progress_bar.update()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.201364  [    0/60000]\n",
      "loss: 1.976470  [ 6400/60000]\n",
      "loss: 1.656949  [12800/60000]\n",
      "loss: 1.551333  [19200/60000]\n",
      "loss: 1.214490  [25600/60000]\n",
      "loss: 1.107938  [32000/60000]\n",
      "loss: 1.051304  [38400/60000]\n",
      "loss: 0.924348  [44800/60000]\n",
      "loss: 0.933896  [51200/60000]\n",
      "loss: 0.852989  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.819374 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.796870  [    0/60000]\n",
      "loss: 0.882160  [ 6400/60000]\n",
      "loss: 0.600098  [12800/60000]\n",
      "loss: 0.860670  [19200/60000]\n",
      "loss: 0.710536  [25600/60000]\n",
      "loss: 0.669946  [32000/60000]\n",
      "loss: 0.732355  [38400/60000]\n",
      "loss: 0.701512  [44800/60000]\n",
      "loss: 0.718694  [51200/60000]\n",
      "loss: 0.664167  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.634836 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.543836  [    0/60000]\n",
      "loss: 0.688140  [ 6400/60000]\n",
      "loss: 0.436789  [12800/60000]\n",
      "loss: 0.730121  [19200/60000]\n",
      "loss: 0.611744  [25600/60000]\n",
      "loss: 0.569436  [32000/60000]\n",
      "loss: 0.622471  [38400/60000]\n",
      "loss: 0.666694  [44800/60000]\n",
      "loss: 0.668495  [51200/60000]\n",
      "loss: 0.562613  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.563933 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.445858  [    0/60000]\n",
      "loss: 0.591430  [ 6400/60000]\n",
      "loss: 0.373286  [12800/60000]\n",
      "loss: 0.665146  [19200/60000]\n",
      "loss: 0.554112  [25600/60000]\n",
      "loss: 0.527444  [32000/60000]\n",
      "loss: 0.558551  [38400/60000]\n",
      "loss: 0.659283  [44800/60000]\n",
      "loss: 0.649230  [51200/60000]\n",
      "loss: 0.499967  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.527077 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.391724  [    0/60000]\n",
      "loss: 0.541265  [ 6400/60000]\n",
      "loss: 0.337559  [12800/60000]\n",
      "loss: 0.622929  [19200/60000]\n",
      "loss: 0.515380  [25600/60000]\n",
      "loss: 0.500299  [32000/60000]\n",
      "loss: 0.518299  [38400/60000]\n",
      "loss: 0.652981  [44800/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-04391d9380b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-ce67babb0da7>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 역전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, epochs)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_relu_stack.0.weight\n",
      "linear_relu_stack.0.bias\n",
      "linear_relu_stack.2.weight_generator.W_l\n",
      "linear_relu_stack.2.weight_generator.W\n",
      "linear_relu_stack.2.weight_generator.B\n",
      "linear_relu_stack.2.bias_generator.W_l\n",
      "linear_relu_stack.2.bias_generator.W\n",
      "linear_relu_stack.2.bias_generator.B\n",
      "linear_relu_stack.3.weight_generator.W_l\n",
      "linear_relu_stack.3.weight_generator.W\n",
      "linear_relu_stack.3.weight_generator.B\n",
      "linear_relu_stack.3.bias_generator.W_l\n",
      "linear_relu_stack.3.bias_generator.W\n",
      "linear_relu_stack.3.bias_generator.B\n",
      "linear_relu_stack.5.weight\n",
      "linear_relu_stack.5.bias\n"
     ]
    }
   ],
   "source": [
    "for name, _ in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: GPU에 올리기\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.318992  [    0/60000]\n",
      "loss: 2.298573  [ 6400/60000]\n",
      "loss: 2.278566  [12800/60000]\n",
      "loss: 2.257627  [19200/60000]\n",
      "loss: 2.259730  [25600/60000]\n",
      "loss: 2.223861  [32000/60000]\n",
      "loss: 2.230925  [38400/60000]\n",
      "loss: 2.206994  [44800/60000]\n",
      "loss: 2.201635  [51200/60000]\n",
      "loss: 2.157983  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 2.157239 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.176568  [    0/60000]\n",
      "loss: 2.162852  [ 6400/60000]\n",
      "loss: 2.111946  [12800/60000]\n",
      "loss: 2.119023  [19200/60000]\n",
      "loss: 2.078768  [25600/60000]\n",
      "loss: 2.014045  [32000/60000]\n",
      "loss: 2.039536  [38400/60000]\n",
      "loss: 1.971239  [44800/60000]\n",
      "loss: 1.965335  [51200/60000]\n",
      "loss: 1.898296  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.895292 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.925843  [    0/60000]\n",
      "loss: 1.901611  [ 6400/60000]\n",
      "loss: 1.794170  [12800/60000]\n",
      "loss: 1.831080  [19200/60000]\n",
      "loss: 1.724633  [25600/60000]\n",
      "loss: 1.666746  [32000/60000]\n",
      "loss: 1.685834  [38400/60000]\n",
      "loss: 1.593378  [44800/60000]\n",
      "loss: 1.607663  [51200/60000]\n",
      "loss: 1.509479  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.525953 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.585624  [    0/60000]\n",
      "loss: 1.559098  [ 6400/60000]\n",
      "loss: 1.419951  [12800/60000]\n",
      "loss: 1.489850  [19200/60000]\n",
      "loss: 1.367522  [25600/60000]\n",
      "loss: 1.352544  [32000/60000]\n",
      "loss: 1.364797  [38400/60000]\n",
      "loss: 1.296702  [44800/60000]\n",
      "loss: 1.328105  [51200/60000]\n",
      "loss: 1.229678  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.257886 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.328282  [    0/60000]\n",
      "loss: 1.317799  [ 6400/60000]\n",
      "loss: 1.165455  [12800/60000]\n",
      "loss: 1.267302  [19200/60000]\n",
      "loss: 1.137664  [25600/60000]\n",
      "loss: 1.153637  [32000/60000]\n",
      "loss: 1.171422  [38400/60000]\n",
      "loss: 1.116249  [44800/60000]\n",
      "loss: 1.156001  [51200/60000]\n",
      "loss: 1.071691  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.093108 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.158641  [    0/60000]\n",
      "loss: 1.167755  [ 6400/60000]\n",
      "loss: 0.998895  [12800/60000]\n",
      "loss: 1.129133  [19200/60000]\n",
      "loss: 0.993942  [25600/60000]\n",
      "loss: 1.021499  [32000/60000]\n",
      "loss: 1.053460  [38400/60000]\n",
      "loss: 1.002422  [44800/60000]\n",
      "loss: 1.044301  [51200/60000]\n",
      "loss: 0.974493  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.987000 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.041117  [    0/60000]\n",
      "loss: 1.071339  [ 6400/60000]\n",
      "loss: 0.885095  [12800/60000]\n",
      "loss: 1.037119  [19200/60000]\n",
      "loss: 0.902624  [25600/60000]\n",
      "loss: 0.929147  [32000/60000]\n",
      "loss: 0.976404  [38400/60000]\n",
      "loss: 0.929014  [44800/60000]\n",
      "loss: 0.966831  [51200/60000]\n",
      "loss: 0.909508  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.914647 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.954092  [    0/60000]\n",
      "loss: 1.004674  [ 6400/60000]\n",
      "loss: 0.803401  [12800/60000]\n",
      "loss: 0.971686  [19200/60000]\n",
      "loss: 0.841496  [25600/60000]\n",
      "loss: 0.861872  [32000/60000]\n",
      "loss: 0.921977  [38400/60000]\n",
      "loss: 0.880159  [44800/60000]\n",
      "loss: 0.910834  [51200/60000]\n",
      "loss: 0.862445  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.862457 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.886875  [    0/60000]\n",
      "loss: 0.954708  [ 6400/60000]\n",
      "loss: 0.741810  [12800/60000]\n",
      "loss: 0.922622  [19200/60000]\n",
      "loss: 0.798012  [25600/60000]\n",
      "loss: 0.811275  [32000/60000]\n",
      "loss: 0.880665  [38400/60000]\n",
      "loss: 0.846316  [44800/60000]\n",
      "loss: 0.868839  [51200/60000]\n",
      "loss: 0.825901  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.822885 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.832885  [    0/60000]\n",
      "loss: 0.914528  [ 6400/60000]\n",
      "loss: 0.693524  [12800/60000]\n",
      "loss: 0.884380  [19200/60000]\n",
      "loss: 0.764823  [25600/60000]\n",
      "loss: 0.772063  [32000/60000]\n",
      "loss: 0.847208  [38400/60000]\n",
      "loss: 0.821532  [44800/60000]\n",
      "loss: 0.836111  [51200/60000]\n",
      "loss: 0.796175  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.791399 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 768),\n",
    "            nn.ReLU(),\n",
    "            StudentLinear('encoder.intermediate.dense', 2, 2, 768, 3072),\n",
    "            StudentLinear('encoder.output.dense', 2, 2, 3072, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.292444  [    0/60000]\n",
      "loss: 1.659353  [ 6400/60000]\n",
      "loss: 1.179987  [12800/60000]\n",
      "loss: 1.211576  [19200/60000]\n",
      "loss: 0.877621  [25600/60000]\n",
      "loss: 0.833095  [32000/60000]\n",
      "loss: 0.842104  [38400/60000]\n",
      "loss: 0.770160  [44800/60000]\n",
      "loss: 0.748130  [51200/60000]\n",
      "loss: 0.741619  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.695336 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.632415  [    0/60000]\n",
      "loss: 0.710258  [ 6400/60000]\n",
      "loss: 0.479420  [12800/60000]\n",
      "loss: 0.786787  [19200/60000]\n",
      "loss: 0.627807  [25600/60000]\n",
      "loss: 0.578678  [32000/60000]\n",
      "loss: 0.658544  [38400/60000]\n",
      "loss: 0.665536  [44800/60000]\n",
      "loss: 0.635270  [51200/60000]\n",
      "loss: 0.581837  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.574849 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.464922  [    0/60000]\n",
      "loss: 0.567500  [ 6400/60000]\n",
      "loss: 0.376697  [12800/60000]\n",
      "loss: 0.675880  [19200/60000]\n",
      "loss: 0.539657  [25600/60000]\n",
      "loss: 0.509184  [32000/60000]\n",
      "loss: 0.574002  [38400/60000]\n",
      "loss: 0.644567  [44800/60000]\n",
      "loss: 0.606817  [51200/60000]\n",
      "loss: 0.500770  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.524420 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.388533  [    0/60000]\n",
      "loss: 0.505800  [ 6400/60000]\n",
      "loss: 0.330643  [12800/60000]\n",
      "loss: 0.612985  [19200/60000]\n",
      "loss: 0.486348  [25600/60000]\n",
      "loss: 0.474936  [32000/60000]\n",
      "loss: 0.518951  [38400/60000]\n",
      "loss: 0.627709  [44800/60000]\n",
      "loss: 0.586402  [51200/60000]\n",
      "loss: 0.458023  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.496043 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.341157  [    0/60000]\n",
      "loss: 0.473270  [ 6400/60000]\n",
      "loss: 0.303120  [12800/60000]\n",
      "loss: 0.569883  [19200/60000]\n",
      "loss: 0.453566  [25600/60000]\n",
      "loss: 0.450837  [32000/60000]\n",
      "loss: 0.479310  [38400/60000]\n",
      "loss: 0.608452  [44800/60000]\n",
      "loss: 0.565632  [51200/60000]\n",
      "loss: 0.435135  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.476956 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.308510  [    0/60000]\n",
      "loss: 0.449362  [ 6400/60000]\n",
      "loss: 0.285904  [12800/60000]\n",
      "loss: 0.537732  [19200/60000]\n",
      "loss: 0.430909  [25600/60000]\n",
      "loss: 0.432580  [32000/60000]\n",
      "loss: 0.451591  [38400/60000]\n",
      "loss: 0.587769  [44800/60000]\n",
      "loss: 0.545411  [51200/60000]\n",
      "loss: 0.421017  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.462872 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.285753  [    0/60000]\n",
      "loss: 0.430643  [ 6400/60000]\n",
      "loss: 0.274113  [12800/60000]\n",
      "loss: 0.512414  [19200/60000]\n",
      "loss: 0.414228  [25600/60000]\n",
      "loss: 0.418910  [32000/60000]\n",
      "loss: 0.430824  [38400/60000]\n",
      "loss: 0.570076  [44800/60000]\n",
      "loss: 0.526573  [51200/60000]\n",
      "loss: 0.411163  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.451496 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.269517  [    0/60000]\n",
      "loss: 0.413821  [ 6400/60000]\n",
      "loss: 0.264761  [12800/60000]\n",
      "loss: 0.492035  [19200/60000]\n",
      "loss: 0.400494  [25600/60000]\n",
      "loss: 0.407730  [32000/60000]\n",
      "loss: 0.414982  [38400/60000]\n",
      "loss: 0.552362  [44800/60000]\n",
      "loss: 0.509784  [51200/60000]\n",
      "loss: 0.405460  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.441944 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.257774  [    0/60000]\n",
      "loss: 0.399742  [ 6400/60000]\n",
      "loss: 0.256927  [12800/60000]\n",
      "loss: 0.475036  [19200/60000]\n",
      "loss: 0.390451  [25600/60000]\n",
      "loss: 0.398838  [32000/60000]\n",
      "loss: 0.401411  [38400/60000]\n",
      "loss: 0.538477  [44800/60000]\n",
      "loss: 0.494193  [51200/60000]\n",
      "loss: 0.400960  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.433751 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.248792  [    0/60000]\n",
      "loss: 0.387356  [ 6400/60000]\n",
      "loss: 0.249182  [12800/60000]\n",
      "loss: 0.459507  [19200/60000]\n",
      "loss: 0.382464  [25600/60000]\n",
      "loss: 0.391187  [32000/60000]\n",
      "loss: 0.388761  [38400/60000]\n",
      "loss: 0.525792  [44800/60000]\n",
      "loss: 0.482162  [51200/60000]\n",
      "loss: 0.397614  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.426887 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
