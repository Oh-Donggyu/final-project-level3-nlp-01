{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from packaging import version\n",
    "from torch.nn import init\n",
    "from tensorly.tenalg import multi_mode_dot\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.file_utils import PushToHubMixin\n",
    "from transformers.modeling_utils import (ModuleUtilsMixin,\n",
    "                                         apply_chunking_to_forward, \n",
    "                                         find_pruneable_heads_and_indices,\n",
    "                                         prune_linear_layer)\n",
    "from transformers.models.bart.modeling_bart import BartEncoderLayer, BartDecoderLayer\n",
    "from transformers.activations import gelu_new\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions, \n",
    "    CausalLMOutputWithCrossAttentions\n",
    ")\n",
    "\n",
    "\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric, load_from_disk\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import set_seed, get_cosine_schedule_with_warmup, AdamW\n",
    "\n",
    "from model import GrafomerModel\n",
    "from utils import preprocess_function_with_setting, load_data, postprocess_text, CustomDataCollator\n",
    "\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.file_utils import PushToHubMixin\n",
    "from transformers.modeling_utils import ModuleUtilsMixin\n",
    "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
    "from transformers.models.bart.modeling_bart import BartEncoderLayer, BartDecoderLayer\n",
    "from transformers.models.bart.modeling_bart import _expand_mask, _make_causal_mask\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "TeacherWeightGroup.set_network(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0593,  0.0244,  0.0095,  ..., -0.0239, -0.0274,  0.0233],\n",
       "        [ 0.1087,  0.0397, -0.0091,  ...,  0.0201, -0.0182, -0.0401],\n",
       "        [ 0.0575, -0.0086,  0.0265,  ..., -0.0381, -0.0725, -0.0275],\n",
       "        ...,\n",
       "        [ 0.0110,  0.0134, -0.0536,  ...,  0.0412, -0.0466,  0.0353],\n",
       "        [ 0.0271, -0.0443,  0.0359,  ..., -0.0067, -0.0521,  0.0614],\n",
       "        [-0.0270, -0.0813, -0.0319,  ...,  0.0358, -0.0717, -0.0470]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WeightGenerator('encoder.attention.self.query', 1, 2, 768, 768)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(768, 768, 6).transpose(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768, 6])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "for _ in range(6):\n",
    "    a = torch.rand(768, 768)\n",
    "    li.append(a)\n",
    "torch.stack(li, dim=-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(768, 768, 6)\n",
    "print(a.size())\n",
    "l = torch.rand(6, 1)\n",
    "a.matmul(l).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "a = torch.rand(768, 768)\n",
    "li.append(a)\n",
    "\n",
    "pooler_out = torch.stack(li, dim=-1)\n",
    "w_l = nn.Parameter(torch.empty(1, 1))\n",
    "out = pooler_out.matmul(w_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4825, 0.3369, 0.1921,  ..., 0.7776, 0.3765, 0.8853],\n",
       "        [0.4086, 0.6695, 0.6508,  ..., 0.5547, 0.7219, 0.8283],\n",
       "        [0.4493, 0.2959, 0.4563,  ..., 0.7574, 0.4172, 0.0586],\n",
       "        ...,\n",
       "        [0.9057, 0.3078, 0.1581,  ..., 0.6746, 0.4088, 0.0066],\n",
       "        [0.8304, 0.8895, 0.7289,  ..., 0.4575, 0.6875, 0.9318],\n",
       "        [0.4120, 0.8666, 0.7388,  ..., 0.9489, 0.7431, 0.4463]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0701e-29, -4.9374e-29, -2.8144e-29,  ..., -1.1394e-28,\n",
       "         -5.5171e-29, -1.2973e-28],\n",
       "        [-5.9882e-29, -9.8099e-29, -9.5361e-29,  ..., -8.1278e-29,\n",
       "         -1.0579e-28, -1.2138e-28],\n",
       "        [-6.5844e-29, -4.3360e-29, -6.6857e-29,  ..., -1.1099e-28,\n",
       "         -6.1142e-29, -8.5914e-30],\n",
       "        ...,\n",
       "        [-1.3272e-28, -4.5098e-29, -2.3160e-29,  ..., -9.8856e-29,\n",
       "         -5.9910e-29, -9.6764e-31],\n",
       "        [-1.2169e-28, -1.3035e-28, -1.0681e-28,  ..., -6.7034e-29,\n",
       "         -1.0075e-28, -1.3654e-28],\n",
       "        [-6.0370e-29, -1.2699e-28, -1.0826e-28,  ..., -1.3904e-28,\n",
       "         -1.0889e-28, -6.5401e-29]], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentLinear(\n",
       "  (weight_generator): WeightGenerator(\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (bias_generator): BiasGenerator(\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StudentLinear('encoder.pooler.dense', 2, 2, 768, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TeacherWeightGroup.set_network(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WeightGenerator('encoder.attention.self.query', 1, 2, 768, 768)().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BiasGenerator('encoder.attention.self.query', 1, 2, 768)().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 768])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WeightGenerator('encoder.intermediate.dense', 2, 2, 768, 3072)().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BiasGenerator('encoder.intermediate.dense', 2, 2, 3072)().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate = StudentLinear('encoder.intermediate.dense', 2, 2, 768, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 3072])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand(1024, 768)\n",
    "intermediate(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 768])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = intermediate(input)\n",
    "output = StudentLinear('encoder.output.dense', 2, 2, 3072, 768)\n",
    "output(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.embeddings.word_embeddings.weight\n",
      "encoder.embeddings.position_embeddings.weight\n",
      "encoder.embeddings.token_type_embeddings.weight\n",
      "encoder.embeddings.LayerNorm.weight\n",
      "encoder.embeddings.LayerNorm.bias\n",
      "encoder.encoder.layer.0.attention.self.query.weight\n",
      "encoder.encoder.layer.0.attention.self.query.bias\n",
      "encoder.encoder.layer.0.attention.self.key.weight\n",
      "encoder.encoder.layer.0.attention.self.key.bias\n",
      "encoder.encoder.layer.0.attention.self.value.weight\n",
      "encoder.encoder.layer.0.attention.self.value.bias\n",
      "encoder.encoder.layer.0.attention.output.dense.weight\n",
      "encoder.encoder.layer.0.attention.output.dense.bias\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.0.intermediate.dense.weight\n",
      "encoder.encoder.layer.0.intermediate.dense.bias\n",
      "encoder.encoder.layer.0.output.dense.weight\n",
      "encoder.encoder.layer.0.output.dense.bias\n",
      "encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.encoder.layer.1.attention.self.query.weight\n",
      "encoder.encoder.layer.1.attention.self.query.bias\n",
      "encoder.encoder.layer.1.attention.self.key.weight\n",
      "encoder.encoder.layer.1.attention.self.key.bias\n",
      "encoder.encoder.layer.1.attention.self.value.weight\n",
      "encoder.encoder.layer.1.attention.self.value.bias\n",
      "encoder.encoder.layer.1.attention.output.dense.weight\n",
      "encoder.encoder.layer.1.attention.output.dense.bias\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.1.intermediate.dense.weight\n",
      "encoder.encoder.layer.1.intermediate.dense.bias\n",
      "encoder.encoder.layer.1.output.dense.weight\n",
      "encoder.encoder.layer.1.output.dense.bias\n",
      "encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.encoder.layer.2.attention.self.query.weight\n",
      "encoder.encoder.layer.2.attention.self.query.bias\n",
      "encoder.encoder.layer.2.attention.self.key.weight\n",
      "encoder.encoder.layer.2.attention.self.key.bias\n",
      "encoder.encoder.layer.2.attention.self.value.weight\n",
      "encoder.encoder.layer.2.attention.self.value.bias\n",
      "encoder.encoder.layer.2.attention.output.dense.weight\n",
      "encoder.encoder.layer.2.attention.output.dense.bias\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.2.intermediate.dense.weight\n",
      "encoder.encoder.layer.2.intermediate.dense.bias\n",
      "encoder.encoder.layer.2.output.dense.weight\n",
      "encoder.encoder.layer.2.output.dense.bias\n",
      "encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.encoder.layer.3.attention.self.query.weight\n",
      "encoder.encoder.layer.3.attention.self.query.bias\n",
      "encoder.encoder.layer.3.attention.self.key.weight\n",
      "encoder.encoder.layer.3.attention.self.key.bias\n",
      "encoder.encoder.layer.3.attention.self.value.weight\n",
      "encoder.encoder.layer.3.attention.self.value.bias\n",
      "encoder.encoder.layer.3.attention.output.dense.weight\n",
      "encoder.encoder.layer.3.attention.output.dense.bias\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.3.intermediate.dense.weight\n",
      "encoder.encoder.layer.3.intermediate.dense.bias\n",
      "encoder.encoder.layer.3.output.dense.weight\n",
      "encoder.encoder.layer.3.output.dense.bias\n",
      "encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.encoder.layer.4.attention.self.query.weight\n",
      "encoder.encoder.layer.4.attention.self.query.bias\n",
      "encoder.encoder.layer.4.attention.self.key.weight\n",
      "encoder.encoder.layer.4.attention.self.key.bias\n",
      "encoder.encoder.layer.4.attention.self.value.weight\n",
      "encoder.encoder.layer.4.attention.self.value.bias\n",
      "encoder.encoder.layer.4.attention.output.dense.weight\n",
      "encoder.encoder.layer.4.attention.output.dense.bias\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.4.intermediate.dense.weight\n",
      "encoder.encoder.layer.4.intermediate.dense.bias\n",
      "encoder.encoder.layer.4.output.dense.weight\n",
      "encoder.encoder.layer.4.output.dense.bias\n",
      "encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.encoder.layer.5.attention.self.query.weight\n",
      "encoder.encoder.layer.5.attention.self.query.bias\n",
      "encoder.encoder.layer.5.attention.self.key.weight\n",
      "encoder.encoder.layer.5.attention.self.key.bias\n",
      "encoder.encoder.layer.5.attention.self.value.weight\n",
      "encoder.encoder.layer.5.attention.self.value.bias\n",
      "encoder.encoder.layer.5.attention.output.dense.weight\n",
      "encoder.encoder.layer.5.attention.output.dense.bias\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.5.intermediate.dense.weight\n",
      "encoder.encoder.layer.5.intermediate.dense.bias\n",
      "encoder.encoder.layer.5.output.dense.weight\n",
      "encoder.encoder.layer.5.output.dense.bias\n",
      "encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.encoder.layer.6.attention.self.query.weight\n",
      "encoder.encoder.layer.6.attention.self.query.bias\n",
      "encoder.encoder.layer.6.attention.self.key.weight\n",
      "encoder.encoder.layer.6.attention.self.key.bias\n",
      "encoder.encoder.layer.6.attention.self.value.weight\n",
      "encoder.encoder.layer.6.attention.self.value.bias\n",
      "encoder.encoder.layer.6.attention.output.dense.weight\n",
      "encoder.encoder.layer.6.attention.output.dense.bias\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.6.intermediate.dense.weight\n",
      "encoder.encoder.layer.6.intermediate.dense.bias\n",
      "encoder.encoder.layer.6.output.dense.weight\n",
      "encoder.encoder.layer.6.output.dense.bias\n",
      "encoder.encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.encoder.layer.7.attention.self.query.weight\n",
      "encoder.encoder.layer.7.attention.self.query.bias\n",
      "encoder.encoder.layer.7.attention.self.key.weight\n",
      "encoder.encoder.layer.7.attention.self.key.bias\n",
      "encoder.encoder.layer.7.attention.self.value.weight\n",
      "encoder.encoder.layer.7.attention.self.value.bias\n",
      "encoder.encoder.layer.7.attention.output.dense.weight\n",
      "encoder.encoder.layer.7.attention.output.dense.bias\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.7.intermediate.dense.weight\n",
      "encoder.encoder.layer.7.intermediate.dense.bias\n",
      "encoder.encoder.layer.7.output.dense.weight\n",
      "encoder.encoder.layer.7.output.dense.bias\n",
      "encoder.encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.encoder.layer.8.attention.self.query.weight\n",
      "encoder.encoder.layer.8.attention.self.query.bias\n",
      "encoder.encoder.layer.8.attention.self.key.weight\n",
      "encoder.encoder.layer.8.attention.self.key.bias\n",
      "encoder.encoder.layer.8.attention.self.value.weight\n",
      "encoder.encoder.layer.8.attention.self.value.bias\n",
      "encoder.encoder.layer.8.attention.output.dense.weight\n",
      "encoder.encoder.layer.8.attention.output.dense.bias\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.8.intermediate.dense.weight\n",
      "encoder.encoder.layer.8.intermediate.dense.bias\n",
      "encoder.encoder.layer.8.output.dense.weight\n",
      "encoder.encoder.layer.8.output.dense.bias\n",
      "encoder.encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.encoder.layer.9.attention.self.query.weight\n",
      "encoder.encoder.layer.9.attention.self.query.bias\n",
      "encoder.encoder.layer.9.attention.self.key.weight\n",
      "encoder.encoder.layer.9.attention.self.key.bias\n",
      "encoder.encoder.layer.9.attention.self.value.weight\n",
      "encoder.encoder.layer.9.attention.self.value.bias\n",
      "encoder.encoder.layer.9.attention.output.dense.weight\n",
      "encoder.encoder.layer.9.attention.output.dense.bias\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.9.intermediate.dense.weight\n",
      "encoder.encoder.layer.9.intermediate.dense.bias\n",
      "encoder.encoder.layer.9.output.dense.weight\n",
      "encoder.encoder.layer.9.output.dense.bias\n",
      "encoder.encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.encoder.layer.10.attention.self.query.weight\n",
      "encoder.encoder.layer.10.attention.self.query.bias\n",
      "encoder.encoder.layer.10.attention.self.key.weight\n",
      "encoder.encoder.layer.10.attention.self.key.bias\n",
      "encoder.encoder.layer.10.attention.self.value.weight\n",
      "encoder.encoder.layer.10.attention.self.value.bias\n",
      "encoder.encoder.layer.10.attention.output.dense.weight\n",
      "encoder.encoder.layer.10.attention.output.dense.bias\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.10.intermediate.dense.weight\n",
      "encoder.encoder.layer.10.intermediate.dense.bias\n",
      "encoder.encoder.layer.10.output.dense.weight\n",
      "encoder.encoder.layer.10.output.dense.bias\n",
      "encoder.encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.encoder.layer.11.attention.self.query.weight\n",
      "encoder.encoder.layer.11.attention.self.query.bias\n",
      "encoder.encoder.layer.11.attention.self.key.weight\n",
      "encoder.encoder.layer.11.attention.self.key.bias\n",
      "encoder.encoder.layer.11.attention.self.value.weight\n",
      "encoder.encoder.layer.11.attention.self.value.bias\n",
      "encoder.encoder.layer.11.attention.output.dense.weight\n",
      "encoder.encoder.layer.11.attention.output.dense.bias\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.11.intermediate.dense.weight\n",
      "encoder.encoder.layer.11.intermediate.dense.bias\n",
      "encoder.encoder.layer.11.output.dense.weight\n",
      "encoder.encoder.layer.11.output.dense.bias\n",
      "encoder.encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.encoder.layer.11.output.LayerNorm.bias\n",
      "encoder.pooler.dense.weight\n",
      "encoder.pooler.dense.bias\n",
      "decoder.transformer.wte.weight\n",
      "decoder.transformer.wpe.weight\n",
      "decoder.transformer.h.0.ln_1.weight\n",
      "decoder.transformer.h.0.ln_1.bias\n",
      "decoder.transformer.h.0.attn.c_attn.weight\n",
      "decoder.transformer.h.0.attn.c_attn.bias\n",
      "decoder.transformer.h.0.attn.c_proj.weight\n",
      "decoder.transformer.h.0.attn.c_proj.bias\n",
      "decoder.transformer.h.0.ln_2.weight\n",
      "decoder.transformer.h.0.ln_2.bias\n",
      "decoder.transformer.h.0.mlp.c_fc.weight\n",
      "decoder.transformer.h.0.mlp.c_fc.bias\n",
      "decoder.transformer.h.0.mlp.c_proj.weight\n",
      "decoder.transformer.h.0.mlp.c_proj.bias\n",
      "decoder.transformer.h.1.ln_1.weight\n",
      "decoder.transformer.h.1.ln_1.bias\n",
      "decoder.transformer.h.1.attn.c_attn.weight\n",
      "decoder.transformer.h.1.attn.c_attn.bias\n",
      "decoder.transformer.h.1.attn.c_proj.weight\n",
      "decoder.transformer.h.1.attn.c_proj.bias\n",
      "decoder.transformer.h.1.ln_2.weight\n",
      "decoder.transformer.h.1.ln_2.bias\n",
      "decoder.transformer.h.1.mlp.c_fc.weight\n",
      "decoder.transformer.h.1.mlp.c_fc.bias\n",
      "decoder.transformer.h.1.mlp.c_proj.weight\n",
      "decoder.transformer.h.1.mlp.c_proj.bias\n",
      "decoder.transformer.h.2.ln_1.weight\n",
      "decoder.transformer.h.2.ln_1.bias\n",
      "decoder.transformer.h.2.attn.c_attn.weight\n",
      "decoder.transformer.h.2.attn.c_attn.bias\n",
      "decoder.transformer.h.2.attn.c_proj.weight\n",
      "decoder.transformer.h.2.attn.c_proj.bias\n",
      "decoder.transformer.h.2.ln_2.weight\n",
      "decoder.transformer.h.2.ln_2.bias\n",
      "decoder.transformer.h.2.mlp.c_fc.weight\n",
      "decoder.transformer.h.2.mlp.c_fc.bias\n",
      "decoder.transformer.h.2.mlp.c_proj.weight\n",
      "decoder.transformer.h.2.mlp.c_proj.bias\n",
      "decoder.transformer.h.3.ln_1.weight\n",
      "decoder.transformer.h.3.ln_1.bias\n",
      "decoder.transformer.h.3.attn.c_attn.weight\n",
      "decoder.transformer.h.3.attn.c_attn.bias\n",
      "decoder.transformer.h.3.attn.c_proj.weight\n",
      "decoder.transformer.h.3.attn.c_proj.bias\n",
      "decoder.transformer.h.3.ln_2.weight\n",
      "decoder.transformer.h.3.ln_2.bias\n",
      "decoder.transformer.h.3.mlp.c_fc.weight\n",
      "decoder.transformer.h.3.mlp.c_fc.bias\n",
      "decoder.transformer.h.3.mlp.c_proj.weight\n",
      "decoder.transformer.h.3.mlp.c_proj.bias\n",
      "decoder.transformer.h.4.ln_1.weight\n",
      "decoder.transformer.h.4.ln_1.bias\n",
      "decoder.transformer.h.4.attn.c_attn.weight\n",
      "decoder.transformer.h.4.attn.c_attn.bias\n",
      "decoder.transformer.h.4.attn.c_proj.weight\n",
      "decoder.transformer.h.4.attn.c_proj.bias\n",
      "decoder.transformer.h.4.ln_2.weight\n",
      "decoder.transformer.h.4.ln_2.bias\n",
      "decoder.transformer.h.4.mlp.c_fc.weight\n",
      "decoder.transformer.h.4.mlp.c_fc.bias\n",
      "decoder.transformer.h.4.mlp.c_proj.weight\n",
      "decoder.transformer.h.4.mlp.c_proj.bias\n",
      "decoder.transformer.h.5.ln_1.weight\n",
      "decoder.transformer.h.5.ln_1.bias\n",
      "decoder.transformer.h.5.attn.c_attn.weight\n",
      "decoder.transformer.h.5.attn.c_attn.bias\n",
      "decoder.transformer.h.5.attn.c_proj.weight\n",
      "decoder.transformer.h.5.attn.c_proj.bias\n",
      "decoder.transformer.h.5.ln_2.weight\n",
      "decoder.transformer.h.5.ln_2.bias\n",
      "decoder.transformer.h.5.mlp.c_fc.weight\n",
      "decoder.transformer.h.5.mlp.c_fc.bias\n",
      "decoder.transformer.h.5.mlp.c_proj.weight\n",
      "decoder.transformer.h.5.mlp.c_proj.bias\n",
      "decoder.transformer.h.6.ln_1.weight\n",
      "decoder.transformer.h.6.ln_1.bias\n",
      "decoder.transformer.h.6.attn.c_attn.weight\n",
      "decoder.transformer.h.6.attn.c_attn.bias\n",
      "decoder.transformer.h.6.attn.c_proj.weight\n",
      "decoder.transformer.h.6.attn.c_proj.bias\n",
      "decoder.transformer.h.6.ln_2.weight\n",
      "decoder.transformer.h.6.ln_2.bias\n",
      "decoder.transformer.h.6.mlp.c_fc.weight\n",
      "decoder.transformer.h.6.mlp.c_fc.bias\n",
      "decoder.transformer.h.6.mlp.c_proj.weight\n",
      "decoder.transformer.h.6.mlp.c_proj.bias\n",
      "decoder.transformer.h.7.ln_1.weight\n",
      "decoder.transformer.h.7.ln_1.bias\n",
      "decoder.transformer.h.7.attn.c_attn.weight\n",
      "decoder.transformer.h.7.attn.c_attn.bias\n",
      "decoder.transformer.h.7.attn.c_proj.weight\n",
      "decoder.transformer.h.7.attn.c_proj.bias\n",
      "decoder.transformer.h.7.ln_2.weight\n",
      "decoder.transformer.h.7.ln_2.bias\n",
      "decoder.transformer.h.7.mlp.c_fc.weight\n",
      "decoder.transformer.h.7.mlp.c_fc.bias\n",
      "decoder.transformer.h.7.mlp.c_proj.weight\n",
      "decoder.transformer.h.7.mlp.c_proj.bias\n",
      "decoder.transformer.h.8.ln_1.weight\n",
      "decoder.transformer.h.8.ln_1.bias\n",
      "decoder.transformer.h.8.attn.c_attn.weight\n",
      "decoder.transformer.h.8.attn.c_attn.bias\n",
      "decoder.transformer.h.8.attn.c_proj.weight\n",
      "decoder.transformer.h.8.attn.c_proj.bias\n",
      "decoder.transformer.h.8.ln_2.weight\n",
      "decoder.transformer.h.8.ln_2.bias\n",
      "decoder.transformer.h.8.mlp.c_fc.weight\n",
      "decoder.transformer.h.8.mlp.c_fc.bias\n",
      "decoder.transformer.h.8.mlp.c_proj.weight\n",
      "decoder.transformer.h.8.mlp.c_proj.bias\n",
      "decoder.transformer.h.9.ln_1.weight\n",
      "decoder.transformer.h.9.ln_1.bias\n",
      "decoder.transformer.h.9.attn.c_attn.weight\n",
      "decoder.transformer.h.9.attn.c_attn.bias\n",
      "decoder.transformer.h.9.attn.c_proj.weight\n",
      "decoder.transformer.h.9.attn.c_proj.bias\n",
      "decoder.transformer.h.9.ln_2.weight\n",
      "decoder.transformer.h.9.ln_2.bias\n",
      "decoder.transformer.h.9.mlp.c_fc.weight\n",
      "decoder.transformer.h.9.mlp.c_fc.bias\n",
      "decoder.transformer.h.9.mlp.c_proj.weight\n",
      "decoder.transformer.h.9.mlp.c_proj.bias\n",
      "decoder.transformer.h.10.ln_1.weight\n",
      "decoder.transformer.h.10.ln_1.bias\n",
      "decoder.transformer.h.10.attn.c_attn.weight\n",
      "decoder.transformer.h.10.attn.c_attn.bias\n",
      "decoder.transformer.h.10.attn.c_proj.weight\n",
      "decoder.transformer.h.10.attn.c_proj.bias\n",
      "decoder.transformer.h.10.ln_2.weight\n",
      "decoder.transformer.h.10.ln_2.bias\n",
      "decoder.transformer.h.10.mlp.c_fc.weight\n",
      "decoder.transformer.h.10.mlp.c_fc.bias\n",
      "decoder.transformer.h.10.mlp.c_proj.weight\n",
      "decoder.transformer.h.10.mlp.c_proj.bias\n",
      "decoder.transformer.h.11.ln_1.weight\n",
      "decoder.transformer.h.11.ln_1.bias\n",
      "decoder.transformer.h.11.attn.c_attn.weight\n",
      "decoder.transformer.h.11.attn.c_attn.bias\n",
      "decoder.transformer.h.11.attn.c_proj.weight\n",
      "decoder.transformer.h.11.attn.c_proj.bias\n",
      "decoder.transformer.h.11.ln_2.weight\n",
      "decoder.transformer.h.11.ln_2.bias\n",
      "decoder.transformer.h.11.mlp.c_fc.weight\n",
      "decoder.transformer.h.11.mlp.c_fc.bias\n",
      "decoder.transformer.h.11.mlp.c_proj.weight\n",
      "decoder.transformer.h.11.mlp.c_proj.bias\n",
      "decoder.transformer.ln_f.weight\n",
      "decoder.transformer.ln_f.bias\n",
      "graft_module.graft_encoder.0.self_attn.k_proj.weight\n",
      "graft_module.graft_encoder.0.self_attn.k_proj.bias\n",
      "graft_module.graft_encoder.0.self_attn.v_proj.weight\n",
      "graft_module.graft_encoder.0.self_attn.v_proj.bias\n",
      "graft_module.graft_encoder.0.self_attn.q_proj.weight\n",
      "graft_module.graft_encoder.0.self_attn.q_proj.bias\n",
      "graft_module.graft_encoder.0.self_attn.out_proj.weight\n",
      "graft_module.graft_encoder.0.self_attn.out_proj.bias\n",
      "graft_module.graft_encoder.0.self_attn_layer_norm.weight\n",
      "graft_module.graft_encoder.0.self_attn_layer_norm.bias\n",
      "graft_module.graft_encoder.0.fc1.weight\n",
      "graft_module.graft_encoder.0.fc1.bias\n",
      "graft_module.graft_encoder.0.fc2.weight\n",
      "graft_module.graft_encoder.0.fc2.bias\n",
      "graft_module.graft_encoder.0.final_layer_norm.weight\n",
      "graft_module.graft_encoder.0.final_layer_norm.bias\n",
      "graft_module.graft_encoder.1.self_attn.k_proj.weight\n",
      "graft_module.graft_encoder.1.self_attn.k_proj.bias\n",
      "graft_module.graft_encoder.1.self_attn.v_proj.weight\n",
      "graft_module.graft_encoder.1.self_attn.v_proj.bias\n",
      "graft_module.graft_encoder.1.self_attn.q_proj.weight\n",
      "graft_module.graft_encoder.1.self_attn.q_proj.bias\n",
      "graft_module.graft_encoder.1.self_attn.out_proj.weight\n",
      "graft_module.graft_encoder.1.self_attn.out_proj.bias\n",
      "graft_module.graft_encoder.1.self_attn_layer_norm.weight\n",
      "graft_module.graft_encoder.1.self_attn_layer_norm.bias\n",
      "graft_module.graft_encoder.1.fc1.weight\n",
      "graft_module.graft_encoder.1.fc1.bias\n",
      "graft_module.graft_encoder.1.fc2.weight\n",
      "graft_module.graft_encoder.1.fc2.bias\n",
      "graft_module.graft_encoder.1.final_layer_norm.weight\n",
      "graft_module.graft_encoder.1.final_layer_norm.bias\n",
      "graft_module.graft_decoder.0.self_attn.k_proj.weight\n",
      "graft_module.graft_decoder.0.self_attn.k_proj.bias\n",
      "graft_module.graft_decoder.0.self_attn.v_proj.weight\n",
      "graft_module.graft_decoder.0.self_attn.v_proj.bias\n",
      "graft_module.graft_decoder.0.self_attn.q_proj.weight\n",
      "graft_module.graft_decoder.0.self_attn.q_proj.bias\n",
      "graft_module.graft_decoder.0.self_attn.out_proj.weight\n",
      "graft_module.graft_decoder.0.self_attn.out_proj.bias\n",
      "graft_module.graft_decoder.0.self_attn_layer_norm.weight\n",
      "graft_module.graft_decoder.0.self_attn_layer_norm.bias\n",
      "graft_module.graft_decoder.0.encoder_attn.k_proj.weight\n",
      "graft_module.graft_decoder.0.encoder_attn.k_proj.bias\n",
      "graft_module.graft_decoder.0.encoder_attn.v_proj.weight\n",
      "graft_module.graft_decoder.0.encoder_attn.v_proj.bias\n",
      "graft_module.graft_decoder.0.encoder_attn.q_proj.weight\n",
      "graft_module.graft_decoder.0.encoder_attn.q_proj.bias\n",
      "graft_module.graft_decoder.0.encoder_attn.out_proj.weight\n",
      "graft_module.graft_decoder.0.encoder_attn.out_proj.bias\n",
      "graft_module.graft_decoder.0.encoder_attn_layer_norm.weight\n",
      "graft_module.graft_decoder.0.encoder_attn_layer_norm.bias\n",
      "graft_module.graft_decoder.0.fc1.weight\n",
      "graft_module.graft_decoder.0.fc1.bias\n",
      "graft_module.graft_decoder.0.fc2.weight\n",
      "graft_module.graft_decoder.0.fc2.bias\n",
      "graft_module.graft_decoder.0.final_layer_norm.weight\n",
      "graft_module.graft_decoder.0.final_layer_norm.bias\n",
      "graft_module.graft_decoder.1.self_attn.k_proj.weight\n",
      "graft_module.graft_decoder.1.self_attn.k_proj.bias\n",
      "graft_module.graft_decoder.1.self_attn.v_proj.weight\n",
      "graft_module.graft_decoder.1.self_attn.v_proj.bias\n",
      "graft_module.graft_decoder.1.self_attn.q_proj.weight\n",
      "graft_module.graft_decoder.1.self_attn.q_proj.bias\n",
      "graft_module.graft_decoder.1.self_attn.out_proj.weight\n",
      "graft_module.graft_decoder.1.self_attn.out_proj.bias\n",
      "graft_module.graft_decoder.1.self_attn_layer_norm.weight\n",
      "graft_module.graft_decoder.1.self_attn_layer_norm.bias\n",
      "graft_module.graft_decoder.1.encoder_attn.k_proj.weight\n",
      "graft_module.graft_decoder.1.encoder_attn.k_proj.bias\n",
      "graft_module.graft_decoder.1.encoder_attn.v_proj.weight\n",
      "graft_module.graft_decoder.1.encoder_attn.v_proj.bias\n",
      "graft_module.graft_decoder.1.encoder_attn.q_proj.weight\n",
      "graft_module.graft_decoder.1.encoder_attn.q_proj.bias\n",
      "graft_module.graft_decoder.1.encoder_attn.out_proj.weight\n",
      "graft_module.graft_decoder.1.encoder_attn.out_proj.bias\n",
      "graft_module.graft_decoder.1.encoder_attn_layer_norm.weight\n",
      "graft_module.graft_decoder.1.encoder_attn_layer_norm.bias\n",
      "graft_module.graft_decoder.1.fc1.weight\n",
      "graft_module.graft_decoder.1.fc1.bias\n",
      "graft_module.graft_decoder.1.fc2.weight\n",
      "graft_module.graft_decoder.1.fc2.bias\n",
      "graft_module.graft_decoder.1.final_layer_norm.weight\n",
      "graft_module.graft_decoder.1.final_layer_norm.bias\n",
      "graft_module.graft_input_pooler.weight\n",
      "graft_module.graft_input_pooler.bias\n",
      "graft_module.graft_output_pooler.weight\n",
      "graft_module.graft_output_pooler.bias\n"
     ]
    }
   ],
   "source": [
    "for name, _ in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherWeightGroup:\n",
    "    teacher_model: nn.Module = None\n",
    "\n",
    "    @classmethod\n",
    "    def set_network(cls, teacher_model: nn.Module):\n",
    "        TeacherWeightGroup.teacher_model = teacher_model\n",
    "\n",
    "    @classmethod\n",
    "    def generate_weight_group(\n",
    "        cls, \n",
    "        weight_class_name: str, \n",
    "        current_layer_index: int, \n",
    "        num_student_layers: int\n",
    "    ):\n",
    "        part, weight_class_name = weight_class_name.split(\".\", 1)\n",
    "        weight_class_name += \".weight\"\n",
    "        weight_instances = list()\n",
    "\n",
    "        if part == \"encoder\":\n",
    "            for instance_name, instance in TeacherWeightGroup.teacher_model.encoder.named_parameters():\n",
    "                if \"attention\" in weight_class_name:\n",
    "                    if weight_class_name in instance_name and 'attention' in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "                else:\n",
    "                    if weight_class_name in instance_name and 'attention' not in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "\n",
    "        elif part == \"decoder\":\n",
    "            for instance_name, instance in TeacherWeightGroup.teacher_model.decoder.named_parameters():\n",
    "                # Conv1d 연산의 파라미터의 in, out이 linear 연산 파라미터의 반대로 되어있음\n",
    "                # 반복문 돌때마다 transpose하지 않고 끝난 후 조정해주면 더 좋을 것 같은데 이후에 시간있으면 수정..\n",
    "                instance = instance.T\n",
    "                if \"attention\" in weight_class_name:\n",
    "                    if weight_class_name in instance_name and 'attention' in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "                else:\n",
    "                    if weight_class_name in instance_name and 'attention' not in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "        \n",
    "        weight_instances = torch.stack(weight_instances, dim=-1)\n",
    "        teacher_network_layers = weight_instances.size()[-1]\n",
    "        \n",
    "        start = (current_layer_index - 1) * int(teacher_network_layers / num_student_layers)\n",
    "        end = current_layer_index * int(teacher_network_layers / num_student_layers)\n",
    "        return weight_instances[:, :, start:end]\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_bias_group(\n",
    "        cls, \n",
    "        weight_class_name: str, \n",
    "        current_layer_index: int, \n",
    "        num_student_layers: int\n",
    "    ):\n",
    "        part, weight_class_name = weight_class_name.split(\".\", 1)\n",
    "        weight_class_name += \".bias\"\n",
    "        weight_instances = list()\n",
    "\n",
    "        if part == \"encoder\":\n",
    "            for instance_name, instance in TeacherWeightGroup.teacher_model.encoder.named_parameters():\n",
    "                if \"attention\" in weight_class_name:\n",
    "                    if weight_class_name in instance_name and 'attention' in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "                else:\n",
    "                    if weight_class_name in instance_name and 'attention' not in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "\n",
    "        elif part == \"decoder\":\n",
    "            for instance_name, instance in TeacherWeightGroup.teacher_model.decoder.named_parameters():\n",
    "                if \"attention\" in weight_class_name:\n",
    "                    if weight_class_name in instance_name and 'attention' in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "                else:\n",
    "                    if weight_class_name in instance_name and 'attention' not in instance_name:\n",
    "                        weight_instances.append(instance)\n",
    "        \n",
    "        weight_instances = torch.stack(weight_instances, dim=-1)\n",
    "        teacher_network_layers = weight_instances.size()[-1]\n",
    "        \n",
    "        start = (current_layer_index - 1) * int(teacher_network_layers / num_student_layers)\n",
    "        end = current_layer_index * int(teacher_network_layers / num_student_layers)\n",
    "        return weight_instances[:, start:end]\n",
    "\n",
    "\n",
    "class WeightGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_class_name: str,\n",
    "        current_layer_index: int,\n",
    "        num_student_layers: int,\n",
    "        student_weight_in: int,\n",
    "        student_weight_out: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.subset = TeacherWeightGroup.generate_weight_group(\n",
    "            weight_class_name, current_layer_index, num_student_layers\n",
    "        )\n",
    "        num_adjacent_layers, teacher_weight_out, teacher_weight_in = self.subset.shape\n",
    "\n",
    "        # self.W_i = nn.Parameter(torch.empty(student_weight_in, teacher_weight_in))\n",
    "        # self.W_o = nn.Parameter(torch.empty(student_weight_out, teacher_weight_out))\n",
    "        self.W_l = nn.Parameter(torch.empty(1, num_adjacent_layers))\n",
    "        self.W = nn.Parameter(torch.ones(student_weight_out, student_weight_in))\n",
    "        self.B = nn.Parameter(torch.zeros(student_weight_out, student_weight_in))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights_()\n",
    "    \n",
    "    def init_weights_(self):\n",
    "        # init.xavier_uniform_(self.W_i)\n",
    "        # init.xavier_uniform_(self.W_o)\n",
    "        init.xavier_uniform_(self.W_l)\n",
    "\n",
    "    def forward(self) -> nn.Parameter :\n",
    "\n",
    "        # mode dot operation of tensorly library\n",
    "        # student_param = multi_mode_dot(self.subset, modes=[2, 1, 0], matrix_or_vec_list=[self.W_i, self.W_o, self.W_l])\n",
    "        student_param = multi_mode_dot(self.subset, modes=[0], matrix_or_vec_list=self.W_l)\n",
    "        return self.tanh(student_param.squeeze(0)) * self.W + self.B\n",
    "\n",
    "# new\n",
    "class WeightGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_class_name: str,\n",
    "        current_layer_index: int,\n",
    "        num_student_layers: int,\n",
    "        student_weight_in: int,\n",
    "        student_weight_out: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.subset = TeacherWeightGroup.generate_weight_group(\n",
    "            weight_class_name, current_layer_index, num_student_layers\n",
    "        )\n",
    "        teacher_weight_out, teacher_weight_in, num_adjacent_layers = self.subset.size()\n",
    "\n",
    "        self.W_l = nn.Parameter(torch.empty(num_adjacent_layers, 1))\n",
    "        self.W = nn.Parameter(torch.ones(student_weight_out, student_weight_in))\n",
    "        self.B = nn.Parameter(torch.zeros(student_weight_out, student_weight_in))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights_()\n",
    "    \n",
    "    def init_weights_(self):\n",
    "        init.xavier_uniform_(self.W_l)\n",
    "\n",
    "    def forward(self) -> nn.Parameter :\n",
    "        student_param = self.subset.matmul(self.W_l)\n",
    "        return self.tanh(student_param.squeeze(-1)) * self.W + self.B\n",
    "\n",
    "\n",
    "class BiasGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_class_name: str,\n",
    "        current_layer_index: int,\n",
    "        num_student_layers: int,\n",
    "        student_out_features: int,\n",
    "    ):\n",
    "\n",
    "        self.subset = TeacherWeightGroup.generate_weight_group(\n",
    "            weight_class_name, current_layer_index, num_student_layers\n",
    "        )\n",
    "        num_adjacent_layers, teacher_out_features = self.subset.shape\n",
    "\n",
    "        # self.W_o = nn.Parameter(torch.empty(student_out_features, teacher_out_features))\n",
    "        self.W_l = nn.Parameter(torch.empty(1, num_adjacent_layers))\n",
    "        self.W = nn.Parameter(torch.ones(student_out_features))\n",
    "        self.B = nn.Parameter(torch.zeros(student_out_features))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights_()\n",
    "    \n",
    "    def init_weights_(self):\n",
    "        # init.xavier_uniform_(self.W_o)\n",
    "        init.xavier_uniform_(self.W_l)\n",
    "    \n",
    "    def forward(self) -> nn.Parameter :\n",
    "\n",
    "        # mode dot operation of tensorly library\n",
    "        # student_param = multi_mode_dot(self.subset, modes=[1, 0], matrix_or_vec_list=[self.W_o, self.W_l])\n",
    "        student_param = multi_mode_dot(self.subset, modes=[0], matrix_or_vec_list=[self.W_l])\n",
    "\n",
    "        return self.tanh(student_param.squeeze(0)) * self.W + self.B\n",
    "\n",
    "# New\n",
    "class BiasGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_class_name: str,\n",
    "        current_layer_index: int,\n",
    "        num_student_layers: int,\n",
    "        student_out_features: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.subset = TeacherWeightGroup.generate_bias_group(\n",
    "            weight_class_name, current_layer_index, num_student_layers\n",
    "        )\n",
    "        teacher_out_features, num_adjacent_layers = self.subset.shape\n",
    "\n",
    "        self.W_l = nn.Parameter(torch.empty(num_adjacent_layers, 1))\n",
    "        self.W = nn.Parameter(torch.ones(student_out_features))\n",
    "        self.B = nn.Parameter(torch.zeros(student_out_features))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights_()\n",
    "    \n",
    "    def init_weights_(self):\n",
    "        init.xavier_uniform_(self.W_l)\n",
    "    \n",
    "    def forward(self) -> nn.Parameter :\n",
    "        student_param = self.subset.matmul(self.W_l)\n",
    "        return self.tanh(student_param.squeeze(-1)) * self.W + self.B\n",
    "\n",
    "class StudentLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        weight_class_name: str, \n",
    "        current_layer_index: int, \n",
    "        num_student_layers: int,\n",
    "        in_features: int, \n",
    "        out_features: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight_generator = WeightGenerator(\n",
    "            weight_class_name = weight_class_name, \n",
    "            current_layer_index = current_layer_index, \n",
    "            num_student_layers = num_student_layers, \n",
    "            student_weight_in = in_features, \n",
    "            student_weight_out = out_features,\n",
    "        )\n",
    "        self.bias_generator = BiasGenerator(\n",
    "            weight_class_name = weight_class_name, \n",
    "            current_layer_index = current_layer_index, \n",
    "            num_student_layers = num_student_layers, \n",
    "            student_out_features = out_features,\n",
    "        )\n",
    "            \n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor :\n",
    "\n",
    "        student_weight = self.weight_generator()\n",
    "        student_bias = self.bias_generator()\n",
    "        \n",
    "        return F.linear(inputs, student_weight, student_bias)\n",
    "\n",
    "class StudentMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        current_layer_index,\n",
    "        num_student_layers,\n",
    "        config\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        intermediate_size = config[\"intermediate_size\"]\n",
    "        \n",
    "        self.c_fc = StudentLinear(\n",
    "            \"decoder.mlp.c_fc\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            hidden_size, \n",
    "            intermediate_size\n",
    "        )\n",
    "        self.c_proj = StudentLinear(\n",
    "            \"decoder.mlp.c_proj\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            intermediate_size, \n",
    "            hidden_size\n",
    "        )\n",
    "        self.act = gelu_new\n",
    "        self.dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.c_fc(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.c_proj(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GraftAttentionModule\n",
    "\n",
    "class GrafomerModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):\n",
    "    def __init__(self, enc_name, dec_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = getattr(__import__(\"transformers\"), 'AutoModel').from_pretrained(enc_name)\n",
    "        self.decoder = getattr(__import__(\"transformers\"), 'AutoModelWithLMHead').from_pretrained(dec_name)\n",
    "\n",
    "        self.config = self.decoder.config  # for compatibility in generate method\n",
    "        self.config.is_encoder_decoder = True\n",
    "        self.config.decoder_start_token_id = 2\n",
    "        print(self.config)\n",
    "\n",
    "        self.decoder_body = getattr(self.decoder, 'transformer')\n",
    "        self.decoder_head = getattr(self.decoder, 'lm_head')\n",
    "\n",
    "        self.bart_config = AutoConfig.from_pretrained(\"facebook/bart-base\")\n",
    "        \n",
    "        self.decoder_embed_dim = 768\n",
    "        self.graft_module_config = {'num_enc_layer': 2, 'num_dec_layer': 2}\n",
    "        self.graft_module = GraftAttentionModule(self.bart_config, self.graft_module_config, self.decoder_embed_dim)\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(input_ids=input_ids,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        head_mask=head_mask,\n",
    "                                        inputs_embeds=inputs_embeds,\n",
    "                                        output_attentions=output_attentions,\n",
    "                                        output_hidden_states=output_hidden_states,\n",
    "                                        return_dict=return_dict)\n",
    "        \n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(last_hidden_state=encoder_outputs[0],\n",
    "                                            hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                                            attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)\n",
    "\n",
    "        # train\n",
    "        if decoder_input_ids is not None:\n",
    "            decoder_outputs = self.decoder_body(input_ids=decoder_input_ids,\n",
    "                                                attention_mask=decoder_attention_mask,\n",
    "                                                use_cache=use_cache)\n",
    "\n",
    "            mask = _expand_mask(attention_mask, self.dtype)\n",
    "            dec_mask = _make_causal_mask(decoder_attention_mask.shape, self.dtype).to(self.device) + _expand_mask(decoder_attention_mask, self.dtype)\n",
    "            cross_mask = _expand_mask(attention_mask, self.dtype, tgt_len=decoder_input_ids.shape[1])\n",
    "        \n",
    "        # eval\n",
    "        else:\n",
    "            decoder_outputs = self.decoder_body(input_ids=input_ids, use_cache=use_cache)\n",
    "\n",
    "            bsz, sql = input_ids.shape\n",
    "            mask = _expand_mask(attention_mask, self.dtype)\n",
    "            dec_mask = _make_causal_mask([bsz, sql], self.dtype).to(self.device)\n",
    "            cross_mask = _expand_mask(attention_mask, self.dtype, tgt_len=sql)\n",
    "\n",
    "\n",
    "        encoder_hidden_state = encoder_outputs[0]\n",
    "        decoder_hidden_state = decoder_outputs[0]\n",
    "\n",
    "        graformer_hidden_state = self.graft_module(\n",
    "            encoder_hidden_states=encoder_hidden_state,\n",
    "            encoder_attention_mask=mask,\n",
    "            decoder_hidden_states=decoder_hidden_state,\n",
    "            decoder_attention_mask=dec_mask,\n",
    "            cross_attention_mask=cross_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        output_hidden_states = decoder_hidden_state + graformer_hidden_state\n",
    "        output_hidden_states = self.decoder_head(output_hidden_states)\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            logits=output_hidden_states\n",
    "        )\n",
    "        \n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        if past is not None:\n",
    "            # print(past, type(past))\n",
    "            # print(len(past))\n",
    "            # print(past.shape)\n",
    "            pass\n",
    "        return {\"input_ids\": input_ids, **kwargs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   146,   112,  ...,     0,     0,     0],\n",
       "        [  101,  4476,  6456,  ...,     0,     0,     0],\n",
       "        [  101, 11937,  1881,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 28140, 20390,  ...,     0,     0,     0],\n",
       "        [  101, 10167, 18873,  ...,     0,     0,     0],\n",
       "        [  101,  8425,  7298,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"kykim/gpt3-kor-small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2Model\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 3,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n",
      "number of model parameters: 330998784\n"
     ]
    }
   ],
   "source": [
    "enc_name = 'bert-base-multilingual-cased'\n",
    "dec_name = 'kykim/gpt3-kor-small_based_on_gpt2'\n",
    "encoder_tokenizer = getattr(__import__(\"transformers\"), \"AutoTokenizer\").from_pretrained(enc_name) # source lang\n",
    "decoder_tokenizer = getattr(__import__(\"transformers\"), \"BertTokenizerFast\").from_pretrained(dec_name)\n",
    "\n",
    "\n",
    "model = GrafomerModel(enc_name, dec_name)\n",
    "model.encoder.load_state_dict(torch.load(\"/opt/ml/final-project-level3-nlp-01/ko_model_checkpoint/ko_encoder_64000.pt\"))\n",
    "model.decoder.load_state_dict(torch.load(\"/opt/ml/final-project-level3-nlp-01/ko_model_checkpoint/ko_decoder_64000.pt\"))\n",
    "model.graft_module.load_state_dict(torch.load(\"/opt/ml/final-project-level3-nlp-01/ko_model_checkpoint/ko_graft_module_64000.pt\"))\n",
    "print(f\"number of model parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(\"cuda:0\")\n",
    "sample = {k: v.to(\"cuda:0\") for k, v in sample.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "                        \n",
    "    # decoder_input_ids = torch.ones((cfg.train_config.batch_size, 1), dtype=torch.long, device=device) * decoder_tokenizer.bos_token_id\n",
    "    generated_tokens = model.generate(sample[\"input_ids\"], attention_mask=sample[\"attention_mask\"], max_length=int(sample[\"input_ids\"].shape[1] * 1.3),\n",
    "                                    # decoder_input_ids=eval_batch[\"decoder_input_ids\"], decoder_attention_mask=eval_batch[\"decoder_attention_mask\"],\n",
    "                                    pad_token_id=decoder_tokenizer.pad_token_id, eos_token_id=decoder_tokenizer.eos_token_id, bos_token_id=decoder_tokenizer.bos_token_id,\n",
    "                                    do_sample=True, top_k=50, top_p=0.95, repetition_penalty=1.2)\n",
    "    labels = sample[\"labels\"]\n",
    "\n",
    "    decoded_preds = decoder_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    decoded_labels = decoder_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '수군수군으로 출세한 그는 사경 ( ) 의 우두머리, 청의 고충을 알게 된 후 서애적객사유상공과 함께 정조약에 의해 복권된 후 서인 세도정치가 그 주역이며 조정에서 이황거 ( ) 와 같은 두 가지 조로 나눠지는 것을 보고 이백량 등 문신, 명척 ( ) 을 초빙해 변론이나 토사의 기전일문 ( ) 에서 구면분답이 일목요연치 않았다.',\n",
       " '한 것은 나약하기 짝이 없는 천하통일이고, 이차적인 것이 될 수 없는 것을 신들림 (, e. b. r. von wilsch : a site. ) 에 대한 사명감부터 가지고 있다.']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['나는 이것들을 나의 자유로운 시간에 써 내려갈 것입니다.'],\n",
       " ['본 고안은 반도체 패키지 몰딩공정용 몰드다이에 관한 것으로서, 구체적으로는 캐비티와 런너사이에 연결 브리지를 별도로 구비하는 반도체 패키지 몰딩공정용 몰드다이에 관한 것이다.'],\n",
       " ['또한, 청지천은 서산의 남동 측 외곽에서 발원하여 북동 측 방향에서 정남 측 방향으로 흐르며, 서산 지역을 통과하는 석림천이 남동 측 방향으로 흘러 청지천으로 유입되고 있다.']]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '수군수군으로 출세한 그는 사경 ( ) 의 우두머리, 청의 고충을 알게 된 후 서애적객사유상공과 함께 정조약에 의해 복권된 후 서인 세도정치가 그 주역이며 조정에서 이황거 ( ) 와 같은 두 가지 조로 나눠지는 것을 보고 이백량 등 문신, 명척 ( ) 을 초빙해 변론이나 토사의 기전일문 ( ) 에서 구면분답이 일목요연치 않았다.',\n",
       " '한 것은 나약하기 짝이 없는 천하통일이고, 이차적인 것이 될 수 없는 것을 신들림 (, e. b. r. von wilsch : a site. ) 에 대한 사명감부터 가지고 있다.',\n",
       " '이 같이 밝혀진 데는 그 ~ 일례로 아따만해.',\n",
       " '.',\n",
       " '##의 고나리 ( ) 가 되어도 될 정도의 이목이 있는 것 같지만, 그 중 의병들에 대한 묘사가 일선 개찰소 및 하급자 ( 혹은 선관위원들 ) 에게까지 영향을 미친다는 것을 들어, 고나리를 극도로 노골화시키기도 한다.',\n",
       " '',\n",
       " '다른 건 다 제쳐두고서라도, 본디 저, 이 정도만 하더라도 너무 좋은 게, 그 자체일 것이오.',\n",
       " '이오시 ( 오상곤 ) 로부터 직접 위자료가 한 푼도 지급될 수 없다는 내용의 통지서를 전해들은 지모 ( 차재원 ) 에 의해 발각돼 거사를 치를 수밖에 없고, 이는 오상의 심복으로 있는 공선무 ( 엄기영 ) 를 무찌른 후 복권을 산 이채원 역시 풀어 준 것으로 전한다.',\n",
       " '',\n",
       " '고인별의 신원화, 명주희 등 동명기체 ( ) 의 불연속적 열람을 가능케 하는 것은 가부동한 이 ( 나치스이 ) 를 위해 사명된 일류작가로 하여금, 사명을 더욱 공고히 한 것이다.',\n",
       " '이백여 명의 인파로 북새통인 가운데 ( 구 한내사우회. 가 ) 도정자, 나문각에 이어 지난 4월에는 ( 사관. 의병장 ) 을 포함한 세 번째 이 같은 자결명분으로 한울이천단 ( 중 ) 과의 연대적 및 항일동지회 소속을 결정하게 됐다.',\n",
       " '',\n",
       " '. 또한, 이번을 통해 그 중 몇 명을 다른 사람들에 대한 이롭게 하는, 타인 - 예지의 장려가 아닌, 일개 고부간 ( ) 에서의 타인이란 한 개인일 수 있는 ( 본디 ) 자와 인 ( ) 의 가 ( ) 에 의하여 만통된 명량해전만리장성만의 가의 ( ) 로서만, 총명하고 가 ( ) 한 만통연하한 천자가 있고, 선한 부마들이 있어 한 ( ) 을 이루며 인 ( ) 을 이룬다는 것이로다 (,, )',\n",
       " '등 수해로 인해 어수선한 분위기일텐데, 정조대원군 일가가 그같은 무위도식적 고단 ( ) 의 아집과 부조리를 부질없게 한 데 대해 사림들의 유감없는 추문이다.',\n",
       " '한 이 같은 반목의 고충은 결국 지난 1일 첫 상조보상인연합을 통해 수임한 것으로 드러나 사견임을 유수했다.']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/envs/lightweight/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:698: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"Chaewon/mnmt_decoder_en\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "number of model parameters: 336556032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/train/cache-3b2cda1dfea1dbb6.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/train/cache-035041f7c0640943.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/train/cache-f7126343de2a6a00.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/train/cache-203a2bf9c2a88ad3.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/train/cache-f176d9fd74456830.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/train/cache-800d358b182d1466.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/train/cache-c984da3cfc47c2ca.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/train/cache-09822e63c6a66341.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/validation/cache-20b01b6fc16e39f6.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/validation/cache-ab42490d356e3581.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/validation/cache-ceb351fba668e919.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/validation/cache-0fa1149bc4cd8eb1.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/validation/cache-18c921a7dee18733.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/validation/cache-e42ddc04c0290c07.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/validation/cache-cb3d490eea6dfa42.arrow\n",
      "Loading cached processed dataset at /opt/ml/final-project-level3-nlp-01/en_unified_dataset/validation/cache-a73a70cf21252103.arrow\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "formatter = logging.Formatter(\"%(asctime)s [%(levelname)s]: %(message)s\")\n",
    "file_handler = logging.FileHandler(filename='train.log')\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "enc_name = 'bert-base-multilingual-cased'\n",
    "dec_name = 'Chaewon/mnmt_decoder_en'\n",
    "encoder_tokenizer = getattr(__import__(\"transformers\"), \"AutoTokenizer\").from_pretrained(enc_name) # source lang\n",
    "decoder_tokenizer = getattr(__import__(\"transformers\"), \"AutoTokenizer\").from_pretrained(dec_name)\n",
    "\n",
    "\n",
    "model = GrafomerModel(enc_name, dec_name)\n",
    "model.encoder.load_state_dict(torch.load(\"/opt/ml/final-project-level3-nlp-01/en_model_checkpoint/en_encoder.pt\"))\n",
    "model.decoder.load_state_dict(torch.load(\"/opt/ml/final-project-level3-nlp-01/en_model_checkpoint/en_decoder.pt\"))\n",
    "model.graft_module.load_state_dict(torch.load(\"/opt/ml/final-project-level3-nlp-01/en_model_checkpoint/en_graft_module.pt\"))\n",
    "print(f\"number of model parameters: {model.num_parameters()}\")\n",
    "\n",
    "# train_set, valid_set = load_data(cfg.train_config.data_path)\n",
    "# raw_dataset = load_from_disk(\"/opt/ml/final-project-level3-nlp-01/ko_unified_dataset\")\n",
    "raw_dataset = load_from_disk(\"/opt/ml/final-project-level3-nlp-01/en_unified_dataset\")\n",
    "train_set = raw_dataset[\"train\"]\n",
    "valid_set = raw_dataset[\"validation\"]\n",
    "\n",
    "fn_kwargs = {'max_length': 512, 'max_target_length': 1024}\n",
    "tokenized_train = train_set.map(preprocess_function_with_setting(encoder_tokenizer, decoder_tokenizer, False, False), \n",
    "                                num_proc=8, batched=True, remove_columns=train_set.column_names, fn_kwargs=fn_kwargs)\n",
    "tokenized_valid = valid_set.map(preprocess_function_with_setting(encoder_tokenizer, decoder_tokenizer, False, False),\n",
    "                                num_proc=8, batched=True, remove_columns=valid_set.column_names, fn_kwargs=fn_kwargs)\n",
    "\n",
    "data_collator = CustomDataCollator(encoder_pad_token_id=encoder_tokenizer.pad_token_id, decoder_pad_token_id=decoder_tokenizer.pad_token_id)\n",
    "train_dataloader = DataLoader(tokenized_train, batch_size=16, pin_memory=True,\n",
    "                                shuffle=True, drop_last=True, num_workers=8, collate_fn=data_collator)\n",
    "valid_dataloader = DataLoader(tokenized_valid, batch_size=16, pin_memory=True, \n",
    "                                shuffle=False, drop_last=False, num_workers=8, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 70])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['decoder_input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 70])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['labels'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")\n",
    "sample = {k: v.to(\"cuda:0\") for k, v in sample.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")\n",
    "sample = {k: v.to(\"cuda:0\") for k, v in sample.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "                        \n",
    "    # decoder_input_ids = torch.ones((cfg.train_config.batch_size, 1), dtype=torch.long, device=device) * decoder_tokenizer.bos_token_id\n",
    "    generated_tokens = model.generate(sample[\"input_ids\"], attention_mask=sample[\"attention_mask\"], max_length=int(sample[\"input_ids\"].shape[1] * 1.3),\n",
    "                                    # decoder_input_ids=eval_batch[\"decoder_input_ids\"], decoder_attention_mask=eval_batch[\"decoder_attention_mask\"],\n",
    "                                    pad_token_id=decoder_tokenizer.pad_token_id, eos_token_id=decoder_tokenizer.eos_token_id, bos_token_id=decoder_tokenizer.bos_token_id,\n",
    "                                    do_sample=True, top_k=50, top_p=0.95, repetition_penalty=1.2)\n",
    "    labels = sample[\"labels\"]\n",
    "\n",
    "    decoded_preds = decoder_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    decoded_labels = decoder_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', '#', '#']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['main issue faced by the Department for General Assembly and Conference Management in the past year had been the servicing of the Human Rights Council and its machinery; while it was fully committed to servicing that body, it could only plan for the workload it generates and summon the required financial and human resources to manage its conferences within the applicable rules, processes and procedures.'],\n",
       " ['iding the merits of the case, the Committee noted the lack of details provided by the complainant concerning his Falun Gong activities and a number of inconsistencies in his account of facts that had undermined the general credibility of his claims, as well as his failure to provide compelling evidence in support of his claim.'],\n",
       " [\"you just got this piece of energy and you just, you're manipulating it.\"],\n",
       " ['performance evaluation area of the consortium joint training center consists of the items of project input, project operation, and project result.'],\n",
       " ['largest increase was the willingness to pay for the 10 m sound barrier, and the smallest increase was for the 15 m sound barrier.'],\n",
       " ['particular, countries in sub-Saharan Africa still lag behind other developing countries.'],\n",
       " ['Inspectors noted in most of the organizations that the number of inactive funds was relatively high, especially in the case of project-related trust funds, and closure of the funds is completed after a long period, without any transactions on the related accounts.'],\n",
       " [', the current debate about religion in the public sphere has largely been driven by proponents of extremes -- those who impose their religious ideology by force and those who deny any place for expressions of faith or belief in the public sphere.'],\n",
       " ['was important, however, for the landlocked and transit developing countries themselves to assume ownership of appropriately designed poverty reduction strategies and national policies that took account of trade and the modernization of their transport sector, in order to complement regional initiatives.'],\n",
       " ['is also worrisome that the InterInstitutional Commission on Human Rights of Indigenous Peoples has not produced results because of lack of continuity and follow-up.'],\n",
       " ['ress Kim Ye-won is mesmerizing the small screen with her hot modern female character.'],\n",
       " ['principle was at the heart of the development agenda, and was fundamental to the work of the United Nations and the success of multilateralism.'],\n",
       " ['that exists as a series of interactions over time?'],\n",
       " [\"if we stop now, you're going to prison, your wife's going to lose her career, and peterson is on his way to being the head of the d.e.a.\"],\n",
       " ['in to your account.'],\n",
       " ['noting the creation of the National Committee to Coordinate the Fight against Drugs chaired by the Prime Minister, the Committee expresses concern about the relatively high use of ecstasy, cocaine, heroin and steroids, by adolescents even at a very early age and the unavailability of drug prevention services that address the needs of adolescent drug users.']]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del sample\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.embeddings.word_embeddings.weight\n",
      "encoder.embeddings.position_embeddings.weight\n",
      "encoder.embeddings.token_type_embeddings.weight\n",
      "encoder.embeddings.LayerNorm.weight\n",
      "encoder.embeddings.LayerNorm.bias\n",
      "encoder.encoder.layer.0.attention.self.query.weight\n",
      "encoder.encoder.layer.0.attention.self.query.bias\n",
      "encoder.encoder.layer.0.attention.self.key.weight\n",
      "encoder.encoder.layer.0.attention.self.key.bias\n",
      "encoder.encoder.layer.0.attention.self.value.weight\n",
      "encoder.encoder.layer.0.attention.self.value.bias\n",
      "encoder.encoder.layer.0.attention.output.dense.weight\n",
      "encoder.encoder.layer.0.attention.output.dense.bias\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.0.intermediate.dense.weight\n",
      "encoder.encoder.layer.0.intermediate.dense.bias\n",
      "encoder.encoder.layer.0.output.dense.weight\n",
      "encoder.encoder.layer.0.output.dense.bias\n",
      "encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.encoder.layer.1.attention.self.query.weight\n",
      "encoder.encoder.layer.1.attention.self.query.bias\n",
      "encoder.encoder.layer.1.attention.self.key.weight\n",
      "encoder.encoder.layer.1.attention.self.key.bias\n",
      "encoder.encoder.layer.1.attention.self.value.weight\n",
      "encoder.encoder.layer.1.attention.self.value.bias\n",
      "encoder.encoder.layer.1.attention.output.dense.weight\n",
      "encoder.encoder.layer.1.attention.output.dense.bias\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.1.intermediate.dense.weight\n",
      "encoder.encoder.layer.1.intermediate.dense.bias\n",
      "encoder.encoder.layer.1.output.dense.weight\n",
      "encoder.encoder.layer.1.output.dense.bias\n",
      "encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.encoder.layer.2.attention.self.query.weight\n",
      "encoder.encoder.layer.2.attention.self.query.bias\n",
      "encoder.encoder.layer.2.attention.self.key.weight\n",
      "encoder.encoder.layer.2.attention.self.key.bias\n",
      "encoder.encoder.layer.2.attention.self.value.weight\n",
      "encoder.encoder.layer.2.attention.self.value.bias\n",
      "encoder.encoder.layer.2.attention.output.dense.weight\n",
      "encoder.encoder.layer.2.attention.output.dense.bias\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.2.intermediate.dense.weight\n",
      "encoder.encoder.layer.2.intermediate.dense.bias\n",
      "encoder.encoder.layer.2.output.dense.weight\n",
      "encoder.encoder.layer.2.output.dense.bias\n",
      "encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.encoder.layer.3.attention.self.query.weight\n",
      "encoder.encoder.layer.3.attention.self.query.bias\n",
      "encoder.encoder.layer.3.attention.self.key.weight\n",
      "encoder.encoder.layer.3.attention.self.key.bias\n",
      "encoder.encoder.layer.3.attention.self.value.weight\n",
      "encoder.encoder.layer.3.attention.self.value.bias\n",
      "encoder.encoder.layer.3.attention.output.dense.weight\n",
      "encoder.encoder.layer.3.attention.output.dense.bias\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.3.intermediate.dense.weight\n",
      "encoder.encoder.layer.3.intermediate.dense.bias\n",
      "encoder.encoder.layer.3.output.dense.weight\n",
      "encoder.encoder.layer.3.output.dense.bias\n",
      "encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.encoder.layer.4.attention.self.query.weight\n",
      "encoder.encoder.layer.4.attention.self.query.bias\n",
      "encoder.encoder.layer.4.attention.self.key.weight\n",
      "encoder.encoder.layer.4.attention.self.key.bias\n",
      "encoder.encoder.layer.4.attention.self.value.weight\n",
      "encoder.encoder.layer.4.attention.self.value.bias\n",
      "encoder.encoder.layer.4.attention.output.dense.weight\n",
      "encoder.encoder.layer.4.attention.output.dense.bias\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.4.intermediate.dense.weight\n",
      "encoder.encoder.layer.4.intermediate.dense.bias\n",
      "encoder.encoder.layer.4.output.dense.weight\n",
      "encoder.encoder.layer.4.output.dense.bias\n",
      "encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.encoder.layer.5.attention.self.query.weight\n",
      "encoder.encoder.layer.5.attention.self.query.bias\n",
      "encoder.encoder.layer.5.attention.self.key.weight\n",
      "encoder.encoder.layer.5.attention.self.key.bias\n",
      "encoder.encoder.layer.5.attention.self.value.weight\n",
      "encoder.encoder.layer.5.attention.self.value.bias\n",
      "encoder.encoder.layer.5.attention.output.dense.weight\n",
      "encoder.encoder.layer.5.attention.output.dense.bias\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.5.intermediate.dense.weight\n",
      "encoder.encoder.layer.5.intermediate.dense.bias\n",
      "encoder.encoder.layer.5.output.dense.weight\n",
      "encoder.encoder.layer.5.output.dense.bias\n",
      "encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.encoder.layer.6.attention.self.query.weight\n",
      "encoder.encoder.layer.6.attention.self.query.bias\n",
      "encoder.encoder.layer.6.attention.self.key.weight\n",
      "encoder.encoder.layer.6.attention.self.key.bias\n",
      "encoder.encoder.layer.6.attention.self.value.weight\n",
      "encoder.encoder.layer.6.attention.self.value.bias\n",
      "encoder.encoder.layer.6.attention.output.dense.weight\n",
      "encoder.encoder.layer.6.attention.output.dense.bias\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.6.intermediate.dense.weight\n",
      "encoder.encoder.layer.6.intermediate.dense.bias\n",
      "encoder.encoder.layer.6.output.dense.weight\n",
      "encoder.encoder.layer.6.output.dense.bias\n",
      "encoder.encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.encoder.layer.7.attention.self.query.weight\n",
      "encoder.encoder.layer.7.attention.self.query.bias\n",
      "encoder.encoder.layer.7.attention.self.key.weight\n",
      "encoder.encoder.layer.7.attention.self.key.bias\n",
      "encoder.encoder.layer.7.attention.self.value.weight\n",
      "encoder.encoder.layer.7.attention.self.value.bias\n",
      "encoder.encoder.layer.7.attention.output.dense.weight\n",
      "encoder.encoder.layer.7.attention.output.dense.bias\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.7.intermediate.dense.weight\n",
      "encoder.encoder.layer.7.intermediate.dense.bias\n",
      "encoder.encoder.layer.7.output.dense.weight\n",
      "encoder.encoder.layer.7.output.dense.bias\n",
      "encoder.encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.encoder.layer.8.attention.self.query.weight\n",
      "encoder.encoder.layer.8.attention.self.query.bias\n",
      "encoder.encoder.layer.8.attention.self.key.weight\n",
      "encoder.encoder.layer.8.attention.self.key.bias\n",
      "encoder.encoder.layer.8.attention.self.value.weight\n",
      "encoder.encoder.layer.8.attention.self.value.bias\n",
      "encoder.encoder.layer.8.attention.output.dense.weight\n",
      "encoder.encoder.layer.8.attention.output.dense.bias\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.8.intermediate.dense.weight\n",
      "encoder.encoder.layer.8.intermediate.dense.bias\n",
      "encoder.encoder.layer.8.output.dense.weight\n",
      "encoder.encoder.layer.8.output.dense.bias\n",
      "encoder.encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.encoder.layer.9.attention.self.query.weight\n",
      "encoder.encoder.layer.9.attention.self.query.bias\n",
      "encoder.encoder.layer.9.attention.self.key.weight\n",
      "encoder.encoder.layer.9.attention.self.key.bias\n",
      "encoder.encoder.layer.9.attention.self.value.weight\n",
      "encoder.encoder.layer.9.attention.self.value.bias\n",
      "encoder.encoder.layer.9.attention.output.dense.weight\n",
      "encoder.encoder.layer.9.attention.output.dense.bias\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.9.intermediate.dense.weight\n",
      "encoder.encoder.layer.9.intermediate.dense.bias\n",
      "encoder.encoder.layer.9.output.dense.weight\n",
      "encoder.encoder.layer.9.output.dense.bias\n",
      "encoder.encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.encoder.layer.10.attention.self.query.weight\n",
      "encoder.encoder.layer.10.attention.self.query.bias\n",
      "encoder.encoder.layer.10.attention.self.key.weight\n",
      "encoder.encoder.layer.10.attention.self.key.bias\n",
      "encoder.encoder.layer.10.attention.self.value.weight\n",
      "encoder.encoder.layer.10.attention.self.value.bias\n",
      "encoder.encoder.layer.10.attention.output.dense.weight\n",
      "encoder.encoder.layer.10.attention.output.dense.bias\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.10.intermediate.dense.weight\n",
      "encoder.encoder.layer.10.intermediate.dense.bias\n",
      "encoder.encoder.layer.10.output.dense.weight\n",
      "encoder.encoder.layer.10.output.dense.bias\n",
      "encoder.encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.encoder.layer.11.attention.self.query.weight\n",
      "encoder.encoder.layer.11.attention.self.query.bias\n",
      "encoder.encoder.layer.11.attention.self.key.weight\n",
      "encoder.encoder.layer.11.attention.self.key.bias\n",
      "encoder.encoder.layer.11.attention.self.value.weight\n",
      "encoder.encoder.layer.11.attention.self.value.bias\n",
      "encoder.encoder.layer.11.attention.output.dense.weight\n",
      "encoder.encoder.layer.11.attention.output.dense.bias\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.11.intermediate.dense.weight\n",
      "encoder.encoder.layer.11.intermediate.dense.bias\n",
      "encoder.encoder.layer.11.output.dense.weight\n",
      "encoder.encoder.layer.11.output.dense.bias\n",
      "encoder.encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.encoder.layer.11.output.LayerNorm.bias\n",
      "encoder.pooler.dense.weight\n",
      "encoder.pooler.dense.bias\n",
      "decoder.transformer.wte.weight\n",
      "decoder.transformer.wpe.weight\n",
      "decoder.transformer.h.0.ln_1.weight\n",
      "decoder.transformer.h.0.ln_1.bias\n",
      "decoder.transformer.h.0.attn.c_attn.weight\n",
      "decoder.transformer.h.0.attn.c_attn.bias\n",
      "decoder.transformer.h.0.attn.c_proj.weight\n",
      "decoder.transformer.h.0.attn.c_proj.bias\n",
      "decoder.transformer.h.0.ln_2.weight\n",
      "decoder.transformer.h.0.ln_2.bias\n",
      "decoder.transformer.h.0.mlp.c_fc.weight\n",
      "decoder.transformer.h.0.mlp.c_fc.bias\n",
      "decoder.transformer.h.0.mlp.c_proj.weight\n",
      "decoder.transformer.h.0.mlp.c_proj.bias\n",
      "decoder.transformer.h.1.ln_1.weight\n",
      "decoder.transformer.h.1.ln_1.bias\n",
      "decoder.transformer.h.1.attn.c_attn.weight\n",
      "decoder.transformer.h.1.attn.c_attn.bias\n",
      "decoder.transformer.h.1.attn.c_proj.weight\n",
      "decoder.transformer.h.1.attn.c_proj.bias\n",
      "decoder.transformer.h.1.ln_2.weight\n",
      "decoder.transformer.h.1.ln_2.bias\n",
      "decoder.transformer.h.1.mlp.c_fc.weight\n",
      "decoder.transformer.h.1.mlp.c_fc.bias\n",
      "decoder.transformer.h.1.mlp.c_proj.weight\n",
      "decoder.transformer.h.1.mlp.c_proj.bias\n",
      "decoder.transformer.h.2.ln_1.weight\n",
      "decoder.transformer.h.2.ln_1.bias\n",
      "decoder.transformer.h.2.attn.c_attn.weight\n",
      "decoder.transformer.h.2.attn.c_attn.bias\n",
      "decoder.transformer.h.2.attn.c_proj.weight\n",
      "decoder.transformer.h.2.attn.c_proj.bias\n",
      "decoder.transformer.h.2.ln_2.weight\n",
      "decoder.transformer.h.2.ln_2.bias\n",
      "decoder.transformer.h.2.mlp.c_fc.weight\n",
      "decoder.transformer.h.2.mlp.c_fc.bias\n",
      "decoder.transformer.h.2.mlp.c_proj.weight\n",
      "decoder.transformer.h.2.mlp.c_proj.bias\n",
      "decoder.transformer.h.3.ln_1.weight\n",
      "decoder.transformer.h.3.ln_1.bias\n",
      "decoder.transformer.h.3.attn.c_attn.weight\n",
      "decoder.transformer.h.3.attn.c_attn.bias\n",
      "decoder.transformer.h.3.attn.c_proj.weight\n",
      "decoder.transformer.h.3.attn.c_proj.bias\n",
      "decoder.transformer.h.3.ln_2.weight\n",
      "decoder.transformer.h.3.ln_2.bias\n",
      "decoder.transformer.h.3.mlp.c_fc.weight\n",
      "decoder.transformer.h.3.mlp.c_fc.bias\n",
      "decoder.transformer.h.3.mlp.c_proj.weight\n",
      "decoder.transformer.h.3.mlp.c_proj.bias\n",
      "decoder.transformer.h.4.ln_1.weight\n",
      "decoder.transformer.h.4.ln_1.bias\n",
      "decoder.transformer.h.4.attn.c_attn.weight\n",
      "decoder.transformer.h.4.attn.c_attn.bias\n",
      "decoder.transformer.h.4.attn.c_proj.weight\n",
      "decoder.transformer.h.4.attn.c_proj.bias\n",
      "decoder.transformer.h.4.ln_2.weight\n",
      "decoder.transformer.h.4.ln_2.bias\n",
      "decoder.transformer.h.4.mlp.c_fc.weight\n",
      "decoder.transformer.h.4.mlp.c_fc.bias\n",
      "decoder.transformer.h.4.mlp.c_proj.weight\n",
      "decoder.transformer.h.4.mlp.c_proj.bias\n",
      "decoder.transformer.h.5.ln_1.weight\n",
      "decoder.transformer.h.5.ln_1.bias\n",
      "decoder.transformer.h.5.attn.c_attn.weight\n",
      "decoder.transformer.h.5.attn.c_attn.bias\n",
      "decoder.transformer.h.5.attn.c_proj.weight\n",
      "decoder.transformer.h.5.attn.c_proj.bias\n",
      "decoder.transformer.h.5.ln_2.weight\n",
      "decoder.transformer.h.5.ln_2.bias\n",
      "decoder.transformer.h.5.mlp.c_fc.weight\n",
      "decoder.transformer.h.5.mlp.c_fc.bias\n",
      "decoder.transformer.h.5.mlp.c_proj.weight\n",
      "decoder.transformer.h.5.mlp.c_proj.bias\n",
      "decoder.transformer.h.6.ln_1.weight\n",
      "decoder.transformer.h.6.ln_1.bias\n",
      "decoder.transformer.h.6.attn.c_attn.weight\n",
      "decoder.transformer.h.6.attn.c_attn.bias\n",
      "decoder.transformer.h.6.attn.c_proj.weight\n",
      "decoder.transformer.h.6.attn.c_proj.bias\n",
      "decoder.transformer.h.6.ln_2.weight\n",
      "decoder.transformer.h.6.ln_2.bias\n",
      "decoder.transformer.h.6.mlp.c_fc.weight\n",
      "decoder.transformer.h.6.mlp.c_fc.bias\n",
      "decoder.transformer.h.6.mlp.c_proj.weight\n",
      "decoder.transformer.h.6.mlp.c_proj.bias\n",
      "decoder.transformer.h.7.ln_1.weight\n",
      "decoder.transformer.h.7.ln_1.bias\n",
      "decoder.transformer.h.7.attn.c_attn.weight\n",
      "decoder.transformer.h.7.attn.c_attn.bias\n",
      "decoder.transformer.h.7.attn.c_proj.weight\n",
      "decoder.transformer.h.7.attn.c_proj.bias\n",
      "decoder.transformer.h.7.ln_2.weight\n",
      "decoder.transformer.h.7.ln_2.bias\n",
      "decoder.transformer.h.7.mlp.c_fc.weight\n",
      "decoder.transformer.h.7.mlp.c_fc.bias\n",
      "decoder.transformer.h.7.mlp.c_proj.weight\n",
      "decoder.transformer.h.7.mlp.c_proj.bias\n",
      "decoder.transformer.h.8.ln_1.weight\n",
      "decoder.transformer.h.8.ln_1.bias\n",
      "decoder.transformer.h.8.attn.c_attn.weight\n",
      "decoder.transformer.h.8.attn.c_attn.bias\n",
      "decoder.transformer.h.8.attn.c_proj.weight\n",
      "decoder.transformer.h.8.attn.c_proj.bias\n",
      "decoder.transformer.h.8.ln_2.weight\n",
      "decoder.transformer.h.8.ln_2.bias\n",
      "decoder.transformer.h.8.mlp.c_fc.weight\n",
      "decoder.transformer.h.8.mlp.c_fc.bias\n",
      "decoder.transformer.h.8.mlp.c_proj.weight\n",
      "decoder.transformer.h.8.mlp.c_proj.bias\n",
      "decoder.transformer.h.9.ln_1.weight\n",
      "decoder.transformer.h.9.ln_1.bias\n",
      "decoder.transformer.h.9.attn.c_attn.weight\n",
      "decoder.transformer.h.9.attn.c_attn.bias\n",
      "decoder.transformer.h.9.attn.c_proj.weight\n",
      "decoder.transformer.h.9.attn.c_proj.bias\n",
      "decoder.transformer.h.9.ln_2.weight\n",
      "decoder.transformer.h.9.ln_2.bias\n",
      "decoder.transformer.h.9.mlp.c_fc.weight\n",
      "decoder.transformer.h.9.mlp.c_fc.bias\n",
      "decoder.transformer.h.9.mlp.c_proj.weight\n",
      "decoder.transformer.h.9.mlp.c_proj.bias\n",
      "decoder.transformer.h.10.ln_1.weight\n",
      "decoder.transformer.h.10.ln_1.bias\n",
      "decoder.transformer.h.10.attn.c_attn.weight\n",
      "decoder.transformer.h.10.attn.c_attn.bias\n",
      "decoder.transformer.h.10.attn.c_proj.weight\n",
      "decoder.transformer.h.10.attn.c_proj.bias\n",
      "decoder.transformer.h.10.ln_2.weight\n",
      "decoder.transformer.h.10.ln_2.bias\n",
      "decoder.transformer.h.10.mlp.c_fc.weight\n",
      "decoder.transformer.h.10.mlp.c_fc.bias\n",
      "decoder.transformer.h.10.mlp.c_proj.weight\n",
      "decoder.transformer.h.10.mlp.c_proj.bias\n",
      "decoder.transformer.h.11.ln_1.weight\n",
      "decoder.transformer.h.11.ln_1.bias\n",
      "decoder.transformer.h.11.attn.c_attn.weight\n",
      "decoder.transformer.h.11.attn.c_attn.bias\n",
      "decoder.transformer.h.11.attn.c_proj.weight\n",
      "decoder.transformer.h.11.attn.c_proj.bias\n",
      "decoder.transformer.h.11.ln_2.weight\n",
      "decoder.transformer.h.11.ln_2.bias\n",
      "decoder.transformer.h.11.mlp.c_fc.weight\n",
      "decoder.transformer.h.11.mlp.c_fc.bias\n",
      "decoder.transformer.h.11.mlp.c_proj.weight\n",
      "decoder.transformer.h.11.mlp.c_proj.bias\n",
      "decoder.transformer.ln_f.weight\n",
      "decoder.transformer.ln_f.bias\n",
      "graft_module.graft_encoder.0.self_attn.k_proj.weight\n",
      "graft_module.graft_encoder.0.self_attn.k_proj.bias\n",
      "graft_module.graft_encoder.0.self_attn.v_proj.weight\n",
      "graft_module.graft_encoder.0.self_attn.v_proj.bias\n",
      "graft_module.graft_encoder.0.self_attn.q_proj.weight\n",
      "graft_module.graft_encoder.0.self_attn.q_proj.bias\n",
      "graft_module.graft_encoder.0.self_attn.out_proj.weight\n",
      "graft_module.graft_encoder.0.self_attn.out_proj.bias\n",
      "graft_module.graft_encoder.0.self_attn_layer_norm.weight\n",
      "graft_module.graft_encoder.0.self_attn_layer_norm.bias\n",
      "graft_module.graft_encoder.0.fc1.weight\n",
      "graft_module.graft_encoder.0.fc1.bias\n",
      "graft_module.graft_encoder.0.fc2.weight\n",
      "graft_module.graft_encoder.0.fc2.bias\n",
      "graft_module.graft_encoder.0.final_layer_norm.weight\n",
      "graft_module.graft_encoder.0.final_layer_norm.bias\n",
      "graft_module.graft_encoder.1.self_attn.k_proj.weight\n",
      "graft_module.graft_encoder.1.self_attn.k_proj.bias\n",
      "graft_module.graft_encoder.1.self_attn.v_proj.weight\n",
      "graft_module.graft_encoder.1.self_attn.v_proj.bias\n",
      "graft_module.graft_encoder.1.self_attn.q_proj.weight\n",
      "graft_module.graft_encoder.1.self_attn.q_proj.bias\n",
      "graft_module.graft_encoder.1.self_attn.out_proj.weight\n",
      "graft_module.graft_encoder.1.self_attn.out_proj.bias\n",
      "graft_module.graft_encoder.1.self_attn_layer_norm.weight\n",
      "graft_module.graft_encoder.1.self_attn_layer_norm.bias\n",
      "graft_module.graft_encoder.1.fc1.weight\n",
      "graft_module.graft_encoder.1.fc1.bias\n",
      "graft_module.graft_encoder.1.fc2.weight\n",
      "graft_module.graft_encoder.1.fc2.bias\n",
      "graft_module.graft_encoder.1.final_layer_norm.weight\n",
      "graft_module.graft_encoder.1.final_layer_norm.bias\n",
      "graft_module.graft_decoder.0.self_attn.k_proj.weight\n",
      "graft_module.graft_decoder.0.self_attn.k_proj.bias\n",
      "graft_module.graft_decoder.0.self_attn.v_proj.weight\n",
      "graft_module.graft_decoder.0.self_attn.v_proj.bias\n",
      "graft_module.graft_decoder.0.self_attn.q_proj.weight\n",
      "graft_module.graft_decoder.0.self_attn.q_proj.bias\n",
      "graft_module.graft_decoder.0.self_attn.out_proj.weight\n",
      "graft_module.graft_decoder.0.self_attn.out_proj.bias\n",
      "graft_module.graft_decoder.0.self_attn_layer_norm.weight\n",
      "graft_module.graft_decoder.0.self_attn_layer_norm.bias\n",
      "graft_module.graft_decoder.0.encoder_attn.k_proj.weight\n",
      "graft_module.graft_decoder.0.encoder_attn.k_proj.bias\n",
      "graft_module.graft_decoder.0.encoder_attn.v_proj.weight\n",
      "graft_module.graft_decoder.0.encoder_attn.v_proj.bias\n",
      "graft_module.graft_decoder.0.encoder_attn.q_proj.weight\n",
      "graft_module.graft_decoder.0.encoder_attn.q_proj.bias\n",
      "graft_module.graft_decoder.0.encoder_attn.out_proj.weight\n",
      "graft_module.graft_decoder.0.encoder_attn.out_proj.bias\n",
      "graft_module.graft_decoder.0.encoder_attn_layer_norm.weight\n",
      "graft_module.graft_decoder.0.encoder_attn_layer_norm.bias\n",
      "graft_module.graft_decoder.0.fc1.weight\n",
      "graft_module.graft_decoder.0.fc1.bias\n",
      "graft_module.graft_decoder.0.fc2.weight\n",
      "graft_module.graft_decoder.0.fc2.bias\n",
      "graft_module.graft_decoder.0.final_layer_norm.weight\n",
      "graft_module.graft_decoder.0.final_layer_norm.bias\n",
      "graft_module.graft_decoder.1.self_attn.k_proj.weight\n",
      "graft_module.graft_decoder.1.self_attn.k_proj.bias\n",
      "graft_module.graft_decoder.1.self_attn.v_proj.weight\n",
      "graft_module.graft_decoder.1.self_attn.v_proj.bias\n",
      "graft_module.graft_decoder.1.self_attn.q_proj.weight\n",
      "graft_module.graft_decoder.1.self_attn.q_proj.bias\n",
      "graft_module.graft_decoder.1.self_attn.out_proj.weight\n",
      "graft_module.graft_decoder.1.self_attn.out_proj.bias\n",
      "graft_module.graft_decoder.1.self_attn_layer_norm.weight\n",
      "graft_module.graft_decoder.1.self_attn_layer_norm.bias\n",
      "graft_module.graft_decoder.1.encoder_attn.k_proj.weight\n",
      "graft_module.graft_decoder.1.encoder_attn.k_proj.bias\n",
      "graft_module.graft_decoder.1.encoder_attn.v_proj.weight\n",
      "graft_module.graft_decoder.1.encoder_attn.v_proj.bias\n",
      "graft_module.graft_decoder.1.encoder_attn.q_proj.weight\n",
      "graft_module.graft_decoder.1.encoder_attn.q_proj.bias\n",
      "graft_module.graft_decoder.1.encoder_attn.out_proj.weight\n",
      "graft_module.graft_decoder.1.encoder_attn.out_proj.bias\n",
      "graft_module.graft_decoder.1.encoder_attn_layer_norm.weight\n",
      "graft_module.graft_decoder.1.encoder_attn_layer_norm.bias\n",
      "graft_module.graft_decoder.1.fc1.weight\n",
      "graft_module.graft_decoder.1.fc1.bias\n",
      "graft_module.graft_decoder.1.fc2.weight\n",
      "graft_module.graft_decoder.1.fc2.bias\n",
      "graft_module.graft_decoder.1.final_layer_norm.weight\n",
      "graft_module.graft_decoder.1.final_layer_norm.bias\n",
      "graft_module.graft_input_pooler.weight\n",
      "graft_module.graft_input_pooler.bias\n",
      "graft_module.graft_output_pooler.weight\n",
      "graft_module.graft_output_pooler.bias\n"
     ]
    }
   ],
   "source": [
    "for name, _ in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_dataloader))\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 75])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 75])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['attention_mask'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GrafomerModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50260, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       "  )\n",
       "  (decoder_body): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       "  (graft_module): GraftAttentionModule(\n",
       "    (graft_encoder): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_decoder): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_input_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (graft_output_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TeacherWeightGroup.set_network(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 119547\n",
       "}"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "class StudencEncoderConfig(BertConfig):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        _name_or_path=\"bert-base-multilingual-cased\",\n",
    "        architectures=[\"BertForMaskedLM\"],\n",
    "        vocab_size=119547,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=4, # Student Layer Length\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        pad_token_id=0,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        use_cache=True,\n",
    "        classifier_dropout=None,\n",
    "        directionality=\"bidi\",\n",
    "        model_type=\"bert\",\n",
    "        pooler_fc_size=768,\n",
    "        pooler_num_attention_heads=12,\n",
    "        pooler_num_fc_layers=3,\n",
    "        pooler_size_per_head=128,\n",
    "        pooler_type=\"first_token_transform\",\n",
    "        transformers_version=\"4.12.5\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "        \n",
    "        self._name_or_path = _name_or_path,\n",
    "        self.architectures = architectures\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        self.use_cache = use_cache\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.directionality = directionality\n",
    "        self.model_type = model_type\n",
    "        self.pooler_fc_size = pooler_fc_size\n",
    "        self.pooler_num_attention_heads = pooler_num_attention_heads\n",
    "        self.pooler_num_fc_layers = pooler_num_fc_layers\n",
    "        self.pooler_size_per_head = pooler_size_per_head\n",
    "        self.pooler_type = pooler_type\n",
    "        self.transformers_version = transformers_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_encoder_config = StudencEncoderConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudencEncoderConfig {\n",
       "  \"_name_or_path\": [\n",
       "    \"bert-base-multilingual-cased\"\n",
       "  ],\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 119547\n",
       "}"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_encoder_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
    "            self.register_buffer(\n",
    "                \"token_type_ids\",\n",
    "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
    "                persistent=False,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
    "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
    "        # issue #5664\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_embeddings = StudentBertEmbeddings(student_encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8294/4243564507.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstudent_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8294/33658896.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "student_embeddings(sample['input_ids']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, current_layer_index, num_student_layers):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = StudentLinear(\n",
    "            \"encoder.attention.self.query\",\n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            config.hidden_size, \n",
    "            self.all_head_size,\n",
    "        )\n",
    "        self.key = StudentLinear(\n",
    "            \"encoder.attention.self.key\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            config.hidden_size, \n",
    "            self.all_head_size,\n",
    "        )\n",
    "        self.value = StudentLinear(\n",
    "            \"encoder.attention.self.value\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            config.hidden_size, \n",
    "            self.all_head_size,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentBertSelfAttention(\n",
       "  (query): StudentLinear(\n",
       "    (weight_generator): WeightGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (bias_generator): BiasGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (key): StudentLinear(\n",
       "    (weight_generator): WeightGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (bias_generator): BiasGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (value): StudentLinear(\n",
       "    (weight_generator): WeightGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (bias_generator): BiasGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StudentBertSelfAttention(student_encoder_config, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertSelfOutput(nn.Module):\n",
    "    def __init__(self, config, current_layer_index, num_student_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense = StudentLinear(\n",
    "            \"encoder.attention.output.dense\",\n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            config.hidden_size, \n",
    "            config.hidden_size,\n",
    "        )\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentBertSelfOutput(\n",
       "  (dense): StudentLinear(\n",
       "    (weight_generator): WeightGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (bias_generator): BiasGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StudentBertSelfOutput(student_encoder_config, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention (self attention + self output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertAttention(nn.Module):\n",
    "    def __init__(self, config, current_layer_index, num_student_layers):\n",
    "        super().__init__()\n",
    "        self.self = StudentBertSelfAttention(config, current_layer_index, num_student_layers)\n",
    "        self.output = StudentBertSelfOutput(config, current_layer_index, num_student_layers)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.query.weight_generator.W_l\n",
      "self.query.weight_generator.W\n",
      "self.query.weight_generator.B\n",
      "self.query.bias_generator.W_l\n",
      "self.query.bias_generator.W\n",
      "self.query.bias_generator.B\n",
      "self.key.weight_generator.W_l\n",
      "self.key.weight_generator.W\n",
      "self.key.weight_generator.B\n",
      "self.key.bias_generator.W_l\n",
      "self.key.bias_generator.W\n",
      "self.key.bias_generator.B\n",
      "self.value.weight_generator.W_l\n",
      "self.value.weight_generator.W\n",
      "self.value.weight_generator.B\n",
      "self.value.bias_generator.W_l\n",
      "self.value.bias_generator.W\n",
      "self.value.bias_generator.B\n",
      "output.dense.weight_generator.W_l\n",
      "output.dense.weight_generator.W\n",
      "output.dense.weight_generator.B\n",
      "output.dense.bias_generator.W_l\n",
      "output.dense.bias_generator.W\n",
      "output.dense.bias_generator.B\n",
      "output.LayerNorm.weight\n",
      "output.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, _ in StudentBertAttention(student_encoder_config, 1, 2).named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate, fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertIntermediate(nn.Module):\n",
    "    def __init__(self, config, current_layer_index, num_student_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = StudentLinear(\n",
    "            \"encoder.intermediate.dense\",\n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            config.hidden_size, \n",
    "            config.intermediate_size,\n",
    "        )\n",
    "\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = nn.functional.gelu\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output, fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertOutput(nn.Module):\n",
    "    def __init__(self, config, current_layer_index, num_student_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense = StudentLinear(\n",
    "            \"encoder.output.dense\",\n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            config.intermediate_size,\n",
    "            config.hidden_size,\n",
    "        )\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertLayer(nn.Module):\n",
    "    def __init__(self, config, current_layer_index, num_student_layers):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = StudentBertAttention(config, current_layer_index, num_student_layers)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
    "            self.crossattention = StudentBertAttention(config, current_layer_index, num_student_layers)\n",
    "        self.intermediate = StudentBertIntermediate(config, current_layer_index, num_student_layers)\n",
    "        self.output = StudentBertOutput(config, current_layer_index, num_student_layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            assert hasattr(\n",
    "                self, \"crossattention\"\n",
    "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention.self.query.weight_generator.W_l\n",
      "attention.self.query.weight_generator.W\n",
      "attention.self.query.weight_generator.B\n",
      "attention.self.query.bias_generator.W_l\n",
      "attention.self.query.bias_generator.W\n",
      "attention.self.query.bias_generator.B\n",
      "attention.self.key.weight_generator.W_l\n",
      "attention.self.key.weight_generator.W\n",
      "attention.self.key.weight_generator.B\n",
      "attention.self.key.bias_generator.W_l\n",
      "attention.self.key.bias_generator.W\n",
      "attention.self.key.bias_generator.B\n",
      "attention.self.value.weight_generator.W_l\n",
      "attention.self.value.weight_generator.W\n",
      "attention.self.value.weight_generator.B\n",
      "attention.self.value.bias_generator.W_l\n",
      "attention.self.value.bias_generator.W\n",
      "attention.self.value.bias_generator.B\n",
      "attention.output.dense.weight_generator.W_l\n",
      "attention.output.dense.weight_generator.W\n",
      "attention.output.dense.weight_generator.B\n",
      "attention.output.dense.bias_generator.W_l\n",
      "attention.output.dense.bias_generator.W\n",
      "attention.output.dense.bias_generator.B\n",
      "attention.output.LayerNorm.weight\n",
      "attention.output.LayerNorm.bias\n",
      "intermediate.dense.weight_generator.W_l\n",
      "intermediate.dense.weight_generator.W\n",
      "intermediate.dense.weight_generator.B\n",
      "intermediate.dense.bias_generator.W_l\n",
      "intermediate.dense.bias_generator.W\n",
      "intermediate.dense.bias_generator.B\n",
      "output.dense.weight_generator.W_l\n",
      "output.dense.weight_generator.W\n",
      "output.dense.weight_generator.B\n",
      "output.dense.bias_generator.W_l\n",
      "output.dense.bias_generator.W\n",
      "output.dense.bias_generator.B\n",
      "output.LayerNorm.weight\n",
      "output.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, _ in StudentBertLayer(student_encoder_config, 1, 2).named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([StudentBertLayer(config, layer_index, config.num_hidden_layers+1) for layer_index in range(1, config.num_hidden_layers+1)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        # if not return_dict:\n",
    "        #     return tuple(\n",
    "        #         v\n",
    "        #         for v in [\n",
    "        #             hidden_states,\n",
    "        #             next_decoder_cache,\n",
    "        #             all_hidden_states,\n",
    "        #             all_self_attentions,\n",
    "        #             all_cross_attentions,\n",
    "        #         ]\n",
    "        #         if v is not None\n",
    "        #     )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentBertEncoder(\n",
       "  (layer): ModuleList(\n",
       "    (0): StudentBertLayer(\n",
       "      (attention): StudentBertAttention(\n",
       "        (self): StudentBertSelfAttention(\n",
       "          (query): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (key): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (value): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): StudentBertSelfOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): StudentBertIntermediate(\n",
       "        (dense): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (output): StudentBertOutput(\n",
       "        (dense): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): StudentBertLayer(\n",
       "      (attention): StudentBertAttention(\n",
       "        (self): StudentBertSelfAttention(\n",
       "          (query): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (key): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (value): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): StudentBertSelfOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): StudentBertIntermediate(\n",
       "        (dense): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (output): StudentBertOutput(\n",
       "        (dense): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): StudentBertLayer(\n",
       "      (attention): StudentBertAttention(\n",
       "        (self): StudentBertSelfAttention(\n",
       "          (query): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (key): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (value): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): StudentBertSelfOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): StudentBertIntermediate(\n",
       "        (dense): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (output): StudentBertOutput(\n",
       "        (dense): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): StudentBertLayer(\n",
       "      (attention): StudentBertAttention(\n",
       "        (self): StudentBertSelfAttention(\n",
       "          (query): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (key): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (value): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): StudentBertSelfOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): StudentBertIntermediate(\n",
       "        (dense): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (output): StudentBertOutput(\n",
       "        (dense): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StudentBertEncoder(student_encoder_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = StudentLinear(\"encoder.pooler.dense\", 1, 1, config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 차원을 바꾸지 않는다면 teacher model의 pooler를 그대로 쓰는게 좋을 것 같아요\n",
    "TeacherWeightGroup.teacher_model.encoder.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 768])"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand(16, 71, 768)\n",
    "\n",
    "teacher_pooler = TeacherWeightGroup.teacher_model.encoder.pooler\n",
    "teacher_pooler(test).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model.encoder.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 125, 768])"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model.encoder.embeddings(sample['input_ids']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentBertModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # self.embeddings = StudentBertEmbeddings(config)\n",
    "        self.embeddings = TeacherWeightGroup.teacher_model.encoder.embeddings\n",
    "        self.encoder = StudentBertEncoder(config)\n",
    "\n",
    "        self.pooler = TeacherWeightGroup.teacher_model.encoder.pooler if add_pooling_layer else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        # self.tie_weigths()\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        # teacher model로부터 불러온 가중치를 초기화하면 안됨 -> nn.Embedding 부분 주석 처리\n",
    "        # StudentLinear는 nn.Linear 인스턴스가 아니므로 초기화되지 않음\n",
    "        # Pooler는 초기화될수도.. -> nn.Linear 부분 주석 처리\n",
    "        # if isinstance(module, nn.Linear):\n",
    "        #     # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "        #     # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "        #     module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        #     if module.bias is not None:\n",
    "        #         module.bias.data.zero_()\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            # module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "    \n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "        \n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "        \n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # (is_decoder... skip ..)\n",
    "        encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        # if not return_dict:\n",
    "        #     return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_encoder = StudentBertModel(student_encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = student_encoder(sample['input_ids'], sample['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 59, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 125, 768])"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.hidden_states[-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 12, 125, 125])"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.attentions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 125, 768])"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 768])"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.pooler_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 125, 768])\n",
      "torch.Size([16, 768])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_output[0].size())\n",
    "print(encoder_output[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer #, BertModel\n",
    "\n",
    "test_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "# model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "test_text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = test_tokenizer(test_text, return_tensors='pt')\n",
    "output = student_encoder(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 72337, 72654, 10911, 10155, 11178, 15541, 13028,   112,   172,\n",
       "         11850,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 768])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([1, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output[0].size())\n",
    "print(output[1].size())\n",
    "print(output[2][-1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "test_encoder = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "encoded_input = test_tokenizer(test_text, return_tensors='pt')\n",
    "output = test_encoder(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output[0].size())\n",
    "print(output[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentBertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): StudentBertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): StudentBertLayer(\n",
       "        (attention): StudentBertAttention(\n",
       "          (self): StudentBertSelfAttention(\n",
       "            (query): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (key): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (value): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): StudentBertSelfOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): StudentBertIntermediate(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (output): StudentBertOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): StudentBertLayer(\n",
       "        (attention): StudentBertAttention(\n",
       "          (self): StudentBertSelfAttention(\n",
       "            (query): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (key): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (value): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): StudentBertSelfOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): StudentBertIntermediate(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (output): StudentBertOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): StudentBertLayer(\n",
       "        (attention): StudentBertAttention(\n",
       "          (self): StudentBertSelfAttention(\n",
       "            (query): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (key): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (value): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): StudentBertSelfOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): StudentBertIntermediate(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (output): StudentBertOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): StudentBertLayer(\n",
       "        (attention): StudentBertAttention(\n",
       "          (self): StudentBertSelfAttention(\n",
       "            (query): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (key): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (value): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): StudentBertSelfOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): StudentBertIntermediate(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (output): StudentBertOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of teacher encoder parameters: 177853440\n",
      "number of student encoder parameters: 149489760\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of teacher encoder parameters: {TeacherWeightGroup.teacher_model.encoder.num_parameters()}\")\n",
    "print(f\"number of student encoder parameters: {student_encoder.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graft Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraftAttentionModule(\n",
       "  (graft_encoder): ModuleList(\n",
       "    (0): BartEncoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): BartEncoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (graft_decoder): ModuleList(\n",
       "    (0): BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (graft_input_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (graft_output_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model.graft_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"Chaewon/mnmt_decoder_en\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50260\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model.decoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentDecoderConfig(GPT2Config):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        _name_or_path=\"kykim/gpt3-kor-small_based_on_gpt2\",\n",
    "        architectures=[\"GPT2Model\"],\n",
    "        vocab_size=42000,\n",
    "        n_positions=2048,\n",
    "        n_ctx=2048,\n",
    "        n_embd=768,\n",
    "        n_layer=4, # = num_hidden_layers = num_student_layers\n",
    "        n_head=12,\n",
    "        n_inner=None,\n",
    "        activation_function=\"gelu_new\",\n",
    "        resid_pdrop=0.1,\n",
    "        embd_pdrop=0.1,\n",
    "        attn_pdrop=0.1,\n",
    "        layer_norm_epsilon=1e-05,\n",
    "        initializer_range=0.02,\n",
    "        summary_type=\"cls_index\",\n",
    "        summary_use_proj=True,\n",
    "        summary_activation=None,\n",
    "        summary_proj_to_labels=True,\n",
    "        summary_first_dropout=0.1,\n",
    "        scale_attn_weights=True,\n",
    "        use_cache=True,\n",
    "        bos_token_id=3,\n",
    "        eos_token_id=3,\n",
    "        transformers_version=\"4.13.0\",\n",
    "        scale_attn_by_inverse_layer_idx=False,\n",
    "        reorder_and_upcast_attn=False,\n",
    "        gradient_checkpointing=False,\n",
    "        decoder_start_token_id=2,\n",
    "        is_encoder_decoder=True,\n",
    "        model_type=\"gpt2\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_positions = n_positions\n",
    "        self.n_embd = n_embd\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_inner = n_inner\n",
    "        self.activation_function = activation_function\n",
    "        self.resid_pdrop = resid_pdrop\n",
    "        self.embd_pdrop = embd_pdrop\n",
    "        self.attn_pdrop = attn_pdrop\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.initializer_range = initializer_range\n",
    "        self.summary_type = summary_type\n",
    "        self.summary_use_proj = summary_use_proj\n",
    "        self.summary_activation = summary_activation\n",
    "        self.summary_first_dropout = summary_first_dropout\n",
    "        self.summary_proj_to_labels = summary_proj_to_labels\n",
    "        self.scale_attn_weights = scale_attn_weights\n",
    "        self.use_cache = use_cache\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.transformers_version = transformers_version,\n",
    "        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx,\n",
    "        self.reorder_and_upcast_attn = reorder_and_upcast_attn,\n",
    "        self.gradient_checkpointing = gradient_checkpointing,\n",
    "        self.decoder_start_token_id = decoder_start_token_id,\n",
    "        self.architectures = architectures,\n",
    "        self._name_or_path = _name_or_path,\n",
    "        self.is_encoder_decoder = is_encoder_decoder,\n",
    "        self.model_type = model_type,\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_decoder_config = StudentDecoderConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kykim/gpt3-kor-small_based_on_gpt2'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_decoder_config._name_or_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentDecoderConfig {\n",
       "  \"_name_or_path\": [\n",
       "    \"kykim/gpt3-kor-small_based_on_gpt2\"\n",
       "  ],\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    [\n",
       "      \"GPT2Model\"\n",
       "    ]\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 3,\n",
       "  \"decoder_start_token_id\": [\n",
       "    2\n",
       "  ],\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"gradient_checkpointing\": [\n",
       "    false\n",
       "  ],\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"is_encoder_decoder\": [\n",
       "    true\n",
       "  ],\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 2048,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 4,\n",
       "  \"n_positions\": 2048,\n",
       "  \"reorder_and_upcast_attn\": [\n",
       "    false\n",
       "  ],\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": [\n",
       "    false\n",
       "  ],\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 42000\n",
       "}"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_decoder_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "768\n",
      "12\n",
      "True\n",
      "0.1\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(student_decoder_config.max_position_embeddings)\n",
    "print(student_decoder_config.hidden_size)\n",
    "print(student_decoder_config.num_attention_heads)\n",
    "print(student_decoder_config.scale_attn_weights)\n",
    "print(student_decoder_config.attn_pdrop)\n",
    "print(student_decoder_config.resid_pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentGPT2Attention(nn.Module):\n",
    "    def __init__(self, config, current_layer_index, num_student_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        max_positions = config.max_position_embeddings\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(\n",
    "                1, 1, max_positions, max_positions\n",
    "            ),\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4))\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.split_size = self.embed_dim\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"`embed_dim` must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        self.scale_attn_weights = config.scale_attn_weights\n",
    "\n",
    "        self.c_attn = StudentLinear(\n",
    "            \"decoder.attn.c_attn\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            self.embed_dim, \n",
    "            3 * self.embed_dim\n",
    "        )\n",
    "        self.c_proj = StudentLinear(\n",
    "            \"decoder.attn.c_proj\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            self.embed_dim, \n",
    "            self.embed_dim\n",
    "        )\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        pass\n",
    "\n",
    "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "\n",
    "        if self.scale_attn_weights:\n",
    "            attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\n",
    "\n",
    "        query_length, key_length = query.size(-2), key.size(-2)\n",
    "        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()\n",
    "        attn_weights = torch.where(causal_mask, attn_weights, self.masked_bias.to(attn_weights.dtype))\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        attn_weights = nn.Softmax(dim=-1)(attn_weights)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "    def _split_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(*new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def _merge_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "        \"\"\"\n",
    "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "        return tensor.view(new_shape)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        layer_past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        if encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"q_attn\"):\n",
    "                raise ValueError(\n",
    "                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n",
    "                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n",
    "                )\n",
    "\n",
    "            query = self.q_attn(hidden_states)\n",
    "            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        else:\n",
    "            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
    "\n",
    "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs  # a, present, (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentGPT2Attention(\n",
       "  (c_attn): StudentLinear(\n",
       "    (weight_generator): WeightGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (bias_generator): BiasGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (c_proj): StudentLinear(\n",
       "    (weight_generator): WeightGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (bias_generator): BiasGenerator(\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StudentGPT2Attention(student_decoder_config, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_1.bias\n",
      "transformer.h.6.attn.c_attn.weight\n",
      "transformer.h.6.attn.c_attn.bias\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.ln_2.weight\n",
      "transformer.h.6.ln_2.bias\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_1.bias\n",
      "transformer.h.7.attn.c_attn.weight\n",
      "transformer.h.7.attn.c_attn.bias\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.ln_2.weight\n",
      "transformer.h.7.ln_2.bias\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_1.bias\n",
      "transformer.h.8.attn.c_attn.weight\n",
      "transformer.h.8.attn.c_attn.bias\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.ln_2.weight\n",
      "transformer.h.8.ln_2.bias\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_1.bias\n",
      "transformer.h.9.attn.c_attn.weight\n",
      "transformer.h.9.attn.c_attn.bias\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.ln_2.weight\n",
      "transformer.h.9.ln_2.bias\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_1.bias\n",
      "transformer.h.10.attn.c_attn.weight\n",
      "transformer.h.10.attn.c_attn.bias\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.ln_2.weight\n",
      "transformer.h.10.ln_2.bias\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_1.bias\n",
      "transformer.h.11.attn.c_attn.weight\n",
      "transformer.h.11.attn.c_attn.bias\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.ln_2.weight\n",
      "transformer.h.11.ln_2.bias\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "for name, _ in TeacherWeightGroup.teacher_model.decoder.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "768\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(student_decoder_config.n_inner)\n",
    "print(student_decoder_config.hidden_size)\n",
    "print(student_decoder_config.resid_pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentGPT2MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 intermediate_size,\n",
    "                 config,\n",
    "                 current_layer_index,\n",
    "                 num_student_layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.c_fc = StudentLinear(\n",
    "            \"decoder.mlp.c_fc\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            embed_dim, \n",
    "            intermediate_size\n",
    "        )\n",
    "        self.c_proj = StudentLinear(\n",
    "            \"decoder.mlp.c_proj\", \n",
    "            current_layer_index, \n",
    "            num_student_layers, \n",
    "            intermediate_size, \n",
    "            embed_dim\n",
    "        )\n",
    "\n",
    "        self.act = gelu_new\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.c_fc(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.c_proj(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "768\n",
      "0.1\n",
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "print(student_decoder_config.n_inner)\n",
    "print(student_decoder_config.hidden_size)\n",
    "print(student_decoder_config.resid_pdrop)\n",
    "print(student_decoder_config.layer_norm_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentGPT2Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 current_layer_index: int,\n",
    "                 num_student_layers: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_size = config.hidden_size\n",
    "        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.attn = StudentGPT2Attention(config, current_layer_index, num_student_layers)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        # if config.add_cross_attention:\n",
    "        #     self.crossattention = GPT2Attention(config, is_cross_attention=True)\n",
    "        #     self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        self.mlp = StudentGPT2MLP(inner_dim, config, current_layer_index, num_student_layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        layer_past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        # if encoder_hidden_states is not None:\n",
    "        #     # add one self-attention block for cross-attention\n",
    "        #     if not hasattr(self, \"crossattention\"):\n",
    "        #         raise ValueError(\n",
    "        #             f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "        #             \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "        #         )\n",
    "        #     residual = hidden_states\n",
    "        #     hidden_states = self.ln_cross_attn(hidden_states)\n",
    "        #     cross_attn_outputs = self.crossattention(\n",
    "        #         hidden_states,\n",
    "        #         attention_mask=attention_mask,\n",
    "        #         head_mask=head_mask,\n",
    "        #         encoder_hidden_states=encoder_hidden_states,\n",
    "        #         encoder_attention_mask=encoder_attention_mask,\n",
    "        #         output_attentions=output_attentions,\n",
    "        #     )\n",
    "        #     attn_output = cross_attn_outputs[0]\n",
    "        #     # residual connection\n",
    "        #     hidden_states = residual + attn_output\n",
    "        #     outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, present, (attentions, cross_attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentGPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): StudentGPT2Attention(\n",
       "    (c_attn): StudentLinear(\n",
       "      (weight_generator): WeightGenerator(\n",
       "        (tanh): Tanh()\n",
       "      )\n",
       "      (bias_generator): BiasGenerator(\n",
       "        (tanh): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (c_proj): StudentLinear(\n",
       "      (weight_generator): WeightGenerator(\n",
       "        (tanh): Tanh()\n",
       "      )\n",
       "      (bias_generator): BiasGenerator(\n",
       "        (tanh): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): StudentGPT2MLP(\n",
       "    (c_fc): StudentLinear(\n",
       "      (weight_generator): WeightGenerator(\n",
       "        (tanh): Tanh()\n",
       "      )\n",
       "      (bias_generator): BiasGenerator(\n",
       "        (tanh): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (c_proj): StudentLinear(\n",
       "      (weight_generator): WeightGenerator(\n",
       "        (tanh): Tanh()\n",
       "      )\n",
       "      (bias_generator): BiasGenerator(\n",
       "        (tanh): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StudentGPT2Block(student_decoder_config, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model.decoder.transformer.wte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1024, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model.decoder.transformer.wpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "42000\n",
      "2048\n",
      "0.1\n",
      "4\n",
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "print(student_decoder_config.hidden_size)\n",
    "print(student_decoder_config.vocab_size)\n",
    "print(student_decoder_config.max_position_embeddings)\n",
    "print(student_decoder_config.embd_pdrop)\n",
    "print(student_decoder_config.num_hidden_layers)\n",
    "print(student_decoder_config.layer_norm_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentGPT2Model(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):\n",
    "    # _keys_to_ignore_on_load_missing = [\"attn.masked_bias\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()#config)\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        # self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        # self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "\n",
    "        # teacher model과 차원이 같다면\n",
    "        self.wte = TeacherWeightGroup.teacher_model.decoder.transformer.wte\n",
    "        self.wpe = TeacherWeightGroup.teacher_model.decoder.transformer.wpe\n",
    "\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.h = nn.ModuleList([StudentGPT2Block(config, layer_index, config.num_hidden_layers+1) for layer_index in range(1, config.num_hidden_layers+1)])\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.wte\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.wte = new_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.h[layer].attn.prune_heads(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "            batch_size = input_ids.shape[0]\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
    "        if position_ids is not None:\n",
    "            position_ids = position_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        if past_key_values is None:\n",
    "            past_length = 0\n",
    "            past_key_values = tuple([None] * len(self.h))\n",
    "        else:\n",
    "            past_length = past_key_values[0][0].size(-2)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "\n",
    "        # GPT2Attention mask.\n",
    "        if attention_mask is not None:\n",
    "            if batch_size <= 0:\n",
    "                raise ValueError(\"batch_size has to be defined and > 0\")\n",
    "            attention_mask = attention_mask.view(batch_size, -1)\n",
    "            # We create a 3D attention mask from a 2D tensor mask.\n",
    "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "            # this attention mask is more simple than the triangular masking of causal attention\n",
    "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "            attention_mask = attention_mask[:, None, None, :]\n",
    "\n",
    "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "            # masked positions, this operation will create a tensor which is 0.0 for\n",
    "            # positions we want to attend and -10000.0 for masked positions.\n",
    "            # Since we are adding it to the raw scores before the softmax, this is\n",
    "            # effectively the same as removing these entirely.\n",
    "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "\n",
    "        # If a 2D ou 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.add_cross_attention and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # head_mask has shape n_layer x batch x n_heads x N x N\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "\n",
    "        if token_type_ids is not None:\n",
    "            token_type_embeds = self.wte(token_type_ids)\n",
    "            hidden_states = hidden_states + token_type_embeds\n",
    "\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        output_shape = input_shape + (hidden_states.size(-1),)\n",
    "\n",
    "        presents = () if use_cache else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
    "\n",
    "            # Model parallel\n",
    "            if self.model_parallel:\n",
    "                torch.cuda.set_device(hidden_states.device)\n",
    "                # Ensure layer_past is on same device as hidden_states (might not be correct)\n",
    "                if layer_past is not None:\n",
    "                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n",
    "                # Ensure that attention_mask is always on the same device as hidden_states\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = attention_mask.to(hidden_states.device)\n",
    "                if isinstance(head_mask, torch.Tensor):\n",
    "                    head_mask = head_mask.to(hidden_states.device)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        # None for past_key_value\n",
    "                        return module(*inputs, use_cache, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(block),\n",
    "                    hidden_states,\n",
    "                    None,\n",
    "                    attention_mask,\n",
    "                    head_mask[i],\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                outputs = block(\n",
    "                    hidden_states,\n",
    "                    layer_past=layer_past,\n",
    "                    attention_mask=attention_mask,\n",
    "                    head_mask=head_mask[i],\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    use_cache=use_cache,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = outputs[0]\n",
    "            if use_cache is True:\n",
    "                presents = presents + (outputs[1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
    "                # if self.config.add_cross_attention:\n",
    "                #     all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
    "\n",
    "            # Model Parallel: If it's the last layer for that device, put things on the next device\n",
    "            if self.model_parallel:\n",
    "                for k, v in self.device_map.items():\n",
    "                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
    "                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
    "\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states.view(*output_shape)\n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )\n",
    "\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=presents,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_decoder = StudentGPT2Model(student_decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 48])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['decoder_input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 48, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embed = student_decoder.wte(sample['decoder_input_ids'])\n",
    "input_embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_attn = StudentLinear(\n",
    "        \"decoder.attn.c_attn\", \n",
    "        1, \n",
    "        4, \n",
    "        768, \n",
    "        3*768\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 48, 2304])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_attn(input_embed).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = student_decoder(sample['decoder_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 48, 768])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.hidden_states[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 12, 48, 48])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.attentions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 62, 768])\n"
     ]
    }
   ],
   "source": [
    "print(decoder_output[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoder_output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentGPT2Model(\n",
       "  (wte): Embedding(50260, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): StudentGPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): StudentGPT2Attention(\n",
       "        (c_attn): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (c_proj): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): StudentGPT2MLP(\n",
       "        (c_fc): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (c_proj): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): StudentGPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): StudentGPT2Attention(\n",
       "        (c_attn): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (c_proj): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): StudentGPT2MLP(\n",
       "        (c_fc): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (c_proj): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): StudentGPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): StudentGPT2Attention(\n",
       "        (c_attn): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (c_proj): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): StudentGPT2MLP(\n",
       "        (c_fc): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (c_proj): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): StudentGPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): StudentGPT2Attention(\n",
       "        (c_attn): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (c_proj): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): StudentGPT2MLP(\n",
       "        (c_fc): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (c_proj): StudentLinear(\n",
       "          (weight_generator): WeightGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "          (bias_generator): BiasGenerator(\n",
       "            (tanh): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GrafomerModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50260, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       "  )\n",
       "  (decoder_body): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       "  (graft_module): GraftAttentionModule(\n",
       "    (graft_encoder): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_decoder): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_input_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (graft_output_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder LMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "42000\n"
     ]
    }
   ],
   "source": [
    "print(student_decoder_config.n_embd)\n",
    "print(student_decoder_config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=50260, bias=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model.decoder.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentGPT2LMHeadModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = StudentGPT2Model(config)\n",
    "        # self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head = TeacherWeightGroup.teacher_model.decoder.lm_head\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.transformer.wte\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "        else:\n",
    "            position_ids = None\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"position_ids\": position_ids,\n",
    "            # \"attention_mask\": attention_mask,\n",
    "            # \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.apply(self._init_weights)\n",
    "        self.tie_weights()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        # if isinstance(module, (nn.Linear, Conv1D)):\n",
    "        #     # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "        #     # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "        #     module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        #     if module.bias is not None:\n",
    "        #         module.bias.data.zero_()\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            # module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=True,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.transformer.first_device)\n",
    "            hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the :obj:`past_key_values` cache if\n",
    "        :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
    "        called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past\n",
    "        )\n",
    "    \n",
    "    def tie_weights(self):\n",
    "        \"\"\"\n",
    "        Tie the weights between the input embeddings and the output embeddings.\n",
    "\n",
    "        If the :obj:`torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning\n",
    "        the weights instead.\n",
    "        \"\"\"\n",
    "        output_embeddings = self.get_output_embeddings()\n",
    "        if output_embeddings is not None and self.config.tie_word_embeddings:\n",
    "            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
    "\n",
    "        if self.config.is_encoder_decoder and self.config.tie_encoder_decoder:\n",
    "            if hasattr(self, self.base_model_prefix):\n",
    "                self = getattr(self, self.base_model_prefix)\n",
    "            self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)\n",
    "\n",
    "        for module in self.modules():\n",
    "            if hasattr(module, \"_tie_weights\"):\n",
    "                module._tie_weights()\n",
    "    \n",
    "    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n",
    "        \"\"\"Tie or clone module weights depending of whether we are using TorchScript or not\"\"\"\n",
    "        if self.config.torchscript:\n",
    "            output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())\n",
    "        else:\n",
    "            output_embeddings.weight = input_embeddings.weight\n",
    "\n",
    "        if getattr(output_embeddings, \"bias\", None) is not None:\n",
    "            output_embeddings.bias.data = nn.functional.pad(\n",
    "                output_embeddings.bias.data,\n",
    "                (\n",
    "                    0,\n",
    "                    output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n",
    "                ),\n",
    "                \"constant\",\n",
    "                0,\n",
    "            )\n",
    "        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n",
    "            output_embeddings.out_features = input_embeddings.num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_decoder_lmhead_model = StudentGPT2LMHeadModel(student_decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = student_decoder_lmhead_model(sample['decoder_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 62, 50260])"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Graformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GrafomerModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50260, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       "  )\n",
       "  (decoder_body): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       "  (graft_module): GraftAttentionModule(\n",
       "    (graft_encoder): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_decoder): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_input_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (graft_output_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeacherWeightGroup.teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentBertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): StudentBertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): StudentBertLayer(\n",
       "        (attention): StudentBertAttention(\n",
       "          (self): StudentBertSelfAttention(\n",
       "            (query): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (key): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (value): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): StudentBertSelfOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): StudentBertIntermediate(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (output): StudentBertOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): StudentBertLayer(\n",
       "        (attention): StudentBertAttention(\n",
       "          (self): StudentBertSelfAttention(\n",
       "            (query): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (key): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (value): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): StudentBertSelfOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): StudentBertIntermediate(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (output): StudentBertOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): StudentBertLayer(\n",
       "        (attention): StudentBertAttention(\n",
       "          (self): StudentBertSelfAttention(\n",
       "            (query): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (key): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (value): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): StudentBertSelfOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): StudentBertIntermediate(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (output): StudentBertOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): StudentBertLayer(\n",
       "        (attention): StudentBertAttention(\n",
       "          (self): StudentBertSelfAttention(\n",
       "            (query): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (key): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (value): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): StudentBertSelfOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): StudentBertIntermediate(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (output): StudentBertOutput(\n",
       "          (dense): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentGPT2LMHeadModel(\n",
       "  (transformer): StudentGPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_decoder_lmhead_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GraftAttentionModule\n",
    "\n",
    "class StudentGrafomerModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):\n",
    "    def __init__(self, enc_name, dec_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = enc_name\n",
    "        self.decoder = dec_name\n",
    "\n",
    "        self.config = self.decoder.config  # for compatibility in generate method\n",
    "        self.config.is_encoder_decoder = True\n",
    "        self.config.decoder_start_token_id = 2\n",
    "        print(self.config)\n",
    "\n",
    "        self.decoder_body = getattr(self.decoder, 'transformer')\n",
    "        self.decoder_head = getattr(self.decoder, 'lm_head')\n",
    "\n",
    "        self.bart_config = AutoConfig.from_pretrained(\"facebook/bart-base\")\n",
    "        \n",
    "        self.decoder_embed_dim = 768\n",
    "        self.graft_module_config = {'num_enc_layer': 2, 'num_dec_layer': 2}\n",
    "        self.graft_module = GraftAttentionModule(self.bart_config, self.graft_module_config, self.decoder_embed_dim)\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(input_ids=input_ids,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        head_mask=head_mask,\n",
    "                                        inputs_embeds=inputs_embeds,\n",
    "                                        output_attentions=output_attentions,\n",
    "                                        output_hidden_states=output_hidden_states,\n",
    "                                        return_dict=return_dict)\n",
    "        \n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(last_hidden_state=encoder_outputs[0],\n",
    "                                            hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                                            attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)\n",
    "\n",
    "        # train\n",
    "        if decoder_input_ids is not None:\n",
    "            decoder_outputs = self.decoder_body(input_ids=decoder_input_ids,\n",
    "                                                attention_mask=decoder_attention_mask,\n",
    "                                                use_cache=use_cache)\n",
    "\n",
    "            mask = _expand_mask(attention_mask, self.dtype)\n",
    "            dec_mask = _make_causal_mask(decoder_attention_mask.shape, self.dtype).to(self.device) + _expand_mask(decoder_attention_mask, self.dtype)\n",
    "            cross_mask = _expand_mask(attention_mask, self.dtype, tgt_len=decoder_input_ids.shape[1])\n",
    "        \n",
    "        # eval\n",
    "        else:\n",
    "            decoder_outputs = self.decoder_body(input_ids=input_ids, use_cache=use_cache)\n",
    "\n",
    "            bsz, sql = input_ids.shape\n",
    "            mask = _expand_mask(attention_mask, self.dtype)\n",
    "            dec_mask = _make_causal_mask([bsz, sql], self.dtype).to(self.device)\n",
    "            cross_mask = _expand_mask(attention_mask, self.dtype, tgt_len=sql)\n",
    "\n",
    "\n",
    "        # Attention\n",
    "        # Hidden layer Output\n",
    "        # Prediction Output\n",
    "        encoder_hidden_state = encoder_outputs[0]\n",
    "        decoder_hidden_state = decoder_outputs[0]\n",
    "        encoder_hidden_states = encoder_outputs.hidden_states\n",
    "        decoder_hidden_states = decoder_outputs.hidden_states\n",
    "        encoder_attentions = encoder_outputs.attentions\n",
    "        decoder_attentions = decoder_outputs.attentions\n",
    "\n",
    "        graformer_hidden_state = self.graft_module(\n",
    "            encoder_hidden_states=encoder_hidden_state,\n",
    "            encoder_attention_mask=mask,\n",
    "            decoder_hidden_states=decoder_hidden_state,\n",
    "            decoder_attention_mask=dec_mask,\n",
    "            cross_attention_mask=cross_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        # print\n",
    "        output_hidden_states = decoder_hidden_state + graformer_hidden_state\n",
    "        output_hidden_states = self.decoder_head(output_hidden_states)\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            logits=output_hidden_states,\n",
    "            encoder_last_hidden_state=encoder_hidden_state,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attentions=encoder_attentions,\n",
    "            # decoder_last_hidden_state=decoder_hidden_state,\n",
    "            decoder_hidden_states=decoder_hidden_states,\n",
    "            decoder_attentions=decoder_attentions,\n",
    "            # graft_hidden_states=graformer_hidden_state,\n",
    "            # graft_attentions=graft_attentions,\n",
    "        )\n",
    "        \n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        if past is not None:\n",
    "            # print(past, type(past))\n",
    "            # print(len(past))\n",
    "            # print(past.shape)\n",
    "            pass\n",
    "        return {\"input_ids\": input_ids, **kwargs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StudentDecoderConfig {\n",
      "  \"_name_or_path\": [\n",
      "    \"kykim/gpt3-kor-small_based_on_gpt2\"\n",
      "  ],\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    [\n",
      "      \"GPT2Model\"\n",
      "    ]\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 3,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gradient_checkpointing\": [\n",
      "    false\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 4,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": [\n",
      "    false\n",
      "  ],\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": [\n",
      "    false\n",
      "  ],\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_grafomer = StudentGrafomerModel(student_encoder, student_decoder_lmhead_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentGrafomerModel(\n",
       "  (encoder): StudentBertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): StudentBertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): StudentBertLayer(\n",
       "          (attention): StudentBertAttention(\n",
       "            (self): StudentBertSelfAttention(\n",
       "              (query): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (key): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (value): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): StudentBertSelfOutput(\n",
       "              (dense): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): StudentBertIntermediate(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): StudentBertOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): StudentBertLayer(\n",
       "          (attention): StudentBertAttention(\n",
       "            (self): StudentBertSelfAttention(\n",
       "              (query): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (key): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (value): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): StudentBertSelfOutput(\n",
       "              (dense): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): StudentBertIntermediate(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): StudentBertOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): StudentBertLayer(\n",
       "          (attention): StudentBertAttention(\n",
       "            (self): StudentBertSelfAttention(\n",
       "              (query): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (key): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (value): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): StudentBertSelfOutput(\n",
       "              (dense): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): StudentBertIntermediate(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): StudentBertOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): StudentBertLayer(\n",
       "          (attention): StudentBertAttention(\n",
       "            (self): StudentBertSelfAttention(\n",
       "              (query): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (key): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (value): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): StudentBertSelfOutput(\n",
       "              (dense): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): StudentBertIntermediate(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): StudentBertOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): StudentGPT2LMHeadModel(\n",
       "    (transformer): StudentGPT2Model(\n",
       "      (wte): Embedding(50260, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): StudentGPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): StudentGPT2Attention(\n",
       "            (c_attn): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): StudentGPT2MLP(\n",
       "            (c_fc): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): StudentGPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): StudentGPT2Attention(\n",
       "            (c_attn): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): StudentGPT2MLP(\n",
       "            (c_fc): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): StudentGPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): StudentGPT2Attention(\n",
       "            (c_attn): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): StudentGPT2MLP(\n",
       "            (c_fc): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): StudentGPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): StudentGPT2Attention(\n",
       "            (c_attn): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): StudentGPT2MLP(\n",
       "            (c_fc): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       "  )\n",
       "  (decoder_body): StudentGPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       "  (graft_module): GraftAttentionModule(\n",
       "    (graft_encoder): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_decoder): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_input_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (graft_output_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_grafomer.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_grafomer_output = student_grafomer(sample[\"input_ids\"], sample[\"attention_mask\"], sample[\"decoder_input_ids\"], sample[\"decoder_attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_grafomer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'decoder_hidden_states', 'decoder_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_grafomer_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_output = model(sample[\"input_ids\"], sample[\"attention_mask\"], sample[\"decoder_input_ids\"], sample[\"decoder_attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class SelectiveDistilationLoss(nn.Module):\n",
    "\n",
    "    queue = deque()\n",
    "\n",
    "    def __init__(self, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.loss_ce_function = nn.NLLLoss(reduction='none', ignore_index=pad_token_id)\n",
    "        self.loss_kd_function = nn.KLDivLoss(reduction='none')\n",
    "\n",
    "    def forward(self, teacher_logits, student_logits, labels, top_r, alpha):\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        loss_ce = self.loss_ce_function(student_logits, labels)\n",
    "        loss_kd = self.loss_kd_function(teacher_logits, student_logits)\n",
    "        \n",
    "        SelectiveDistilationLoss.queue.extend(torch.tensor(list(zip(labels, loss_ce))))\n",
    "\n",
    "        while len(SelectiveDistilationLoss.queue) > 50000:\n",
    "            SelectiveDistilationLoss.queue.popleft()\n",
    "        \n",
    "        hard_threshold = int(len(SelectiveDistilationLoss.queue) * top_r) # 0 < top_r <= 1.0\n",
    "        sorted_queue = sorted(SelectiveDistilationLoss.queue, key=lambda x: x[1], reverse=True)\n",
    "        hard_words = set(map(lambda x: x[0], sorted_queue[:hard_threshold]))\n",
    "\n",
    "        for label, loss_ce, loss_kd in zip(labels, loss_ce, loss_kd):\n",
    "            if label in hard_words:\n",
    "                print(\"hard\")\n",
    "                loss += loss_ce + alpha * loss_kd\n",
    "            else:\n",
    "                print(\"not hard\")\n",
    "                loss += loss_ce\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "class AllDistillationLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.loss_ce_function = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "        self.loss_kd_function = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    def forward(self, teacher_logits, student_logits, labels, alpha):\n",
    "\n",
    "        loss_ce = self.loss_ce_function(student_logits, labels)\n",
    "        loss_kd = self.loss_kd_function(teacher_logits, student_logits)\n",
    "\n",
    "        return loss_ce + (alpha * loss_kd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = AllDistillationLoss(pad_token_id=3)\n",
    "\n",
    "student_logits = student_grafomer_output.logits.view(-1, len(decoder_tokenizer))\n",
    "teacher_logits = teacher_output.logits.view(-1, len(decoder_tokenizer))\n",
    "labels = sample[\"labels\"].view(-1)\n",
    "\n",
    "loss = loss_function(teacher_logits, student_logits, labels, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2197.3611, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n",
      "not hard\n"
     ]
    }
   ],
   "source": [
    "loss_function = SelectiveDistilationLoss(pad_token_id=3)\n",
    "\n",
    "student_logits = student_grafomer_output.logits.view(-1, len(decoder_tokenizer))\n",
    "teacher_logits = teacher_output.logits.view(-1, len(decoder_tokenizer))\n",
    "labels = sample[\"labels\"].view(-1)\n",
    "\n",
    "loss = loss_function(teacher_logits, student_logits, labels, 0.7, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-19.6120, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 12, 125, 125])"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_grafomer_output.encoder_attentions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 62, 50260])"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_grafomer_output[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 62, 50260])"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_grafomer_output.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_grafomer_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentGrafomerModel(\n",
       "  (encoder): StudentBertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): StudentBertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): StudentBertLayer(\n",
       "          (attention): StudentBertAttention(\n",
       "            (self): StudentBertSelfAttention(\n",
       "              (query): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (key): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (value): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): StudentBertSelfOutput(\n",
       "              (dense): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): StudentBertIntermediate(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): StudentBertOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): StudentBertLayer(\n",
       "          (attention): StudentBertAttention(\n",
       "            (self): StudentBertSelfAttention(\n",
       "              (query): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (key): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (value): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): StudentBertSelfOutput(\n",
       "              (dense): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): StudentBertIntermediate(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): StudentBertOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): StudentBertLayer(\n",
       "          (attention): StudentBertAttention(\n",
       "            (self): StudentBertSelfAttention(\n",
       "              (query): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (key): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (value): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): StudentBertSelfOutput(\n",
       "              (dense): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): StudentBertIntermediate(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): StudentBertOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): StudentBertLayer(\n",
       "          (attention): StudentBertAttention(\n",
       "            (self): StudentBertSelfAttention(\n",
       "              (query): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (key): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (value): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): StudentBertSelfOutput(\n",
       "              (dense): StudentLinear(\n",
       "                (weight_generator): WeightGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "                (bias_generator): BiasGenerator(\n",
       "                  (tanh): Tanh()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): StudentBertIntermediate(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): StudentBertOutput(\n",
       "            (dense): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): StudentGPT2LMHeadModel(\n",
       "    (transformer): StudentGPT2Model(\n",
       "      (wte): Embedding(42000, 768)\n",
       "      (wpe): Embedding(2048, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): StudentGPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): StudentGPT2Attention(\n",
       "            (c_attn): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): StudentGPT2MLP(\n",
       "            (c_fc): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): StudentGPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): StudentGPT2Attention(\n",
       "            (c_attn): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): StudentGPT2MLP(\n",
       "            (c_fc): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): StudentGPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): StudentGPT2Attention(\n",
       "            (c_attn): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): StudentGPT2MLP(\n",
       "            (c_fc): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): StudentGPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): StudentGPT2Attention(\n",
       "            (c_attn): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): StudentGPT2MLP(\n",
       "            (c_fc): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (c_proj): StudentLinear(\n",
       "              (weight_generator): WeightGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "              (bias_generator): BiasGenerator(\n",
       "                (tanh): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=42000, bias=False)\n",
       "  )\n",
       "  (decoder_body): StudentGPT2Model(\n",
       "    (wte): Embedding(42000, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): StudentGPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): StudentGPT2Attention(\n",
       "          (c_attn): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): StudentGPT2MLP(\n",
       "          (c_fc): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (c_proj): StudentLinear(\n",
       "            (weight_generator): WeightGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "            (bias_generator): BiasGenerator(\n",
       "              (tanh): Tanh()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder_head): Linear(in_features=768, out_features=42000, bias=False)\n",
       "  (graft_module): GraftAttentionModule(\n",
       "    (graft_encoder): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_decoder): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (graft_input_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (graft_output_pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_grafomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
